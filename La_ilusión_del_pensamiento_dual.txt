A ver, creo que casi todos hemos oído hablar de esta idea de que tenemos como dos formas de pensar, ¿no?
El famoso Sistema 1 y Sistema 2.
Sí, claro.
Uno es el piloto automático, el rápido, el intuitivo.
Y el otro es, pues, el piloto manual.
Lento, analítico, el que usas para armar un mueble de esos complicados.
La verdad es que es un marco súper popular porque parece que lo explica casi todo.
Pero bueno, el material que tenemos hoy sobre la mesa lo pone todo patas arriba.
Le da una vuelta de tuerca interesante.
Totalmente.
Es un artículo académico, El mito de la cognición dual, y algunas notas que lo explican.
Proponen algo que suena casi a herejía, que esa división en dos sistemas es en realidad un espejismo.
Un error conceptual.
Sí.
Así que nuestra misión hoy es meternos de lleno en esta idea que llaman teoría de la relegación de aspectos
y ver si de verdad cambia cómo entendemos la intuición, el aprendizaje y, sobre todo, el debate actual sobre la inteligencia artificial.
Y es importante aclarar algo desde el principio, ¿no?
La teoría no dice que no sintamos que hay pensamientos rápidos y otros lentos.
Eso es real, lo sentimos todos.
Claro.
Lo que argumenta es que el sistema 1 no es un motor diferente en el cerebro.
Es más bien el resultado de procesos que antes eran lentos del sistema 2, pero que con la práctica se han comprimido, se han automatizado.
O sea que la intuición no es magia.
Exacto. La intuición es el razonamiento de siempre, pero con todo el andamiaje, todo el paso a paso ya oculto.
A ver, vamos a desmenuzar esa idea un poco.
El artículo habla de una deriva ontológica. La frase suena, bueno, intimidante, la verdad.
Sí, suena a clase de filosofía.
Estamos diciendo que los científicos básicamente se creyeron demasiado su propia metáfora.
Es exactamente eso. No hay mejor forma de ponerlo. La deriva ontológica es ese resbalón, ese paso de una descripción útil a una afirmación sobre cómo es la realidad.
Ajá.
Es como, no sé, describir el día y la noche y de repente concluir que hay dos soles diferentes, uno para cada momento.
Se empezó a hablar del sistema 1 y 2 como si fueran dos máquinas, dos computadoras separadas dentro del cráneo.
Y claro, si crees que son dos máquinas distintas, empiezas a tratarlas distinto.
Precisamente. Y los textos señalan dos consecuencias de esto que son bastante serias.
La primera es una especie de moralización del pensamiento.
¿Moralización? ¿Cómo así?
Sí. El sistema 1 pasa a ser el villano, el impulsivo, el que nos mete en problemas, el poco fiable, ¿no?
El irracional.
Eso. Y el sistema 2 es el héroe, el racional, el correcto, al que siempre deberíamos aspirar.
Le ponemos una etiqueta de bueno y malo a nuestros propios procesos mentales.
Entiendo. ¿Y la segunda consecuencia?
La segunda es que nos lleva a hacer juicios muy simplistas sobre la inteligencia. Y aquí es donde entra la ía.
Claro.
Si el comportamiento de un sistema parece rápido, fluido, automático, pues lo metemos en la caja del sistema 1 y lo descartamos.
Decimos, ah, eso es solo reconocimiento de patrones, no es razonamiento de verdad.
Se asume que es superficial porque no le costó trabajo.
Exactamente.
Pero si esa división es un mito, ¿cuál es la alternativa?
La teoría propone esta idea de relegación. Y el ejemplo que usan, el del viaje diario al trabajo, me parece genial.
Es perfecto, sí.
Al principio, cuando empiezas en un trabajo nuevo, piensas en cada detalle. Salgo de casa, camino a la parada, me fijo en qué estación bajar. Es un esfuerzo consciente.
Totalmente.
Y después de un mes, ¿qué pasa?
Que lo haces sin pensar. Llegas al trabajo y casi no recuerdas el viaje. Tu cuerpo te lleva solo.
Ahí está. Esa es la relegación de aspectos. A medida que la ruta se vuelve familiar y exitosa, porque llegas a tiempo, todos esos pequeños detalles que antes ocupaban tu atención se van, relegando, se ocultan, se comprimen.
Se ejecutan en segundo plano.
Justo. El razonamiento no desapareció, ¿eh? Su estructura interna se hizo invisible para ahorrarte energía. El cerebro es muy eficiente. Dice, esto funciona. No necesito redibujar el mapa cada mañana.
O sea que no es un interruptor que cambia de modo lento a modo rápido. Es más como hacer zoom en un mapa.
Mmm, me gusta esa analogía.
A veces, en una ciudad nueva, necesitas ver el mapa con todo el detalle de las calles. Pero en tu propia ciudad, solo necesitas ver la ruta general. El punto A y el punto B.
Esa imagen del mapa es buenísima, sí. Y lo más importante que los textos subrayan es que ese zoom es reversible.
¡Ah, claro!
Imagina que en tu viaje diario, de repente tu estación de metro habitual está cerrada. ¿Qué pasa?
Pues que el piloto automático se apaga de golpe. Tienes que parar, pensar, sacar el móvil, buscar otra ruta.
Exacto. El sistema repromueve los aspectos que había relegado. El zoom se acerca de nuevo, los detalles vuelven a tu conciencia y deliberas.
¿Y eso qué se siente como un cambio de sistema 1 a sistema 2?
En realidad es un ajuste en la resolución con la que estás mirando el problema. No encendiste un motor nuevo. Le dijiste el que ya tenías. Oye, mira esto con más detalle. Porque el plan A falló.
Un momento. El texto dice algo aquí que me acaba de hacer clic. La intuición es el razonamiento de ayer, eficientemente olvidado.
Es una frase genial, ¿no?
¿Me estás diciendo que mi corazonada sobre algo es en realidad el eco de un montón de trabajo mental que ya hice y ni siquiera recuerdo?
Eso es. Y es fascinante porque desmitifica la intuición sin quitarle ni un gramo de su poder.
A ver.
La intuición de una médica experta que ve a un paciente y sabe que algo anda mal. O la de un ajedrecista que ve la jugada ganadora en un segundo. No sale de la nada.
Claro, viene de la experiencia.
Es el producto de miles de horas de razonamiento, de prueba y error, de análisis, que se ha comprimido tanto que su ejecución se siente instantánea.
La sensación de que la respuesta simplemente te llega es porque los pasos intermedios ya no son visibles, no porque nunca existieron.
Y eso también cambia la idea del esfuerzo. Sentir que algo te cuesta mucho trabajo no significa necesariamente que estés pensando mejor.
Para nada. El esfuerzo cognitivo es solo una señal. Es el termómetro que te indica que estás manteniendo muchos detalles, muchas distinciones activas en tu conciencia al mismo tiempo.
Cuando te vuelves experta en algo, el esfuerzo disminuye. Pero no porque la tarea se haya vuelto más simple, sino porque has reorganizado su complejidad de una forma mucho más eficiente.
Ok. La teoría es elegante, pero se me ocurre una objeción obvia.
A ver.
¿Qué pasa con las cosas que son automáticas desde que nacemos? Un reflejo, el miedo a las alturas, esos procesos nunca fueron deliberados para que pudiéramos relegarlos.
Es una pregunta clave y la teoría la aborda de frente. Distingue entre dos tipos de automatismo.
Ah, ok.
Por un lado, está el que llamas cableado o innato. Es el producto de millones de años de evolución.
El que ya viene de fábrica.
Eso. Y por otro lado, está el automatismo relegado. El que se adquiere con la práctica, como hablar tu idioma o andar en bicicleta.
Y el argumento central aquí es que los procesos que de verdad nos interesan para los debates sobre racionalidad, entender un sarcasmo, planificar un viaje, interpretar a otros, todos pertenecen a esa segunda categoría.
Son habilidades aprendidas.
Entendido.
Por lo tanto, el modelo de relegación se aplica perfectamente a las formas de cognición más complejas.
Bien, y con esto aterrizamos en el campo de batalla de hoy. La inteligencia artificial.
Entiendo la teoría, pero seamos honestos. Cuando ves un LLM cometer un error tonto y obvio, se siente como un fallo de Sistema 1.
Totalmente.
La crítica de gente como Gary Marcus, que dice que solo son loros estadísticos, resuene mucho con esa experiencia. ¿Cómo responde esta teoría a esa sensación?
Esa es la reacción intuitiva. Y es justo el tipo de pensamiento que esta teoría busca explicar.
Me atrapaste.
El punto no es que los LLMs no cometan errores. Cometen muchísimos. El problema es que etiquetar ese error como un fallo de Sistema 1 es un error de categoría. Nos impide ver el verdadero problema.
¿Y cuál sería la enfermedad entonces y no el síntoma?
Si vemos el Sistema 1 como un Sistema 2 comprimido, entonces la velocidad y fluidez de un LLM no son prueba de que le falte razonamiento. Al contrario.
Podrían ser prueba de lo contrario.
Podrían ser indicadores de una comprensión muy exitosa. El artículo usa una analogía brutal. Decir que un LLM no razona porque es rápido es como mirar el código ya compilado de un programa, ese montón de ceros y unos, y argumentar que eso no es computación de verdad, ignorando el lenguaje de programación que lo generó.
Entendido. Es una perspectiva muy diferente. Entonces, si la limitación no es que le falte un motor de Sistema 2, ¿cuál es el verdadero punto débil de la IA actual según esta visión?
Su punto débil es algo que los investigadores llaman con un nombre bastante técnico. La falta de mecanismos endógenos robustos para regular la resolución representacional.
Vale. Esa frase sí que necesita una traducción.
Totalmente. En español simple significa que a la IA le falta esa vocecita interna que todos tenemos.
Ajá.
La que de repente nos dice, espera un momento, mi respuesta automática, mi intuición, no encaja aquí. Algo no cuadra. Necesito analizar esto con más cuidado.
Le falta la capacidad de dudar de sí misma.
Exacto. Carecen de la capacidad de detectar por sí mismos cuando su conocimiento comprimido es insuficiente o inapropiado. No saben cuándo necesitan hacer zoom. Por eso, el gran reto para construir una IA más avanzada no sería atornillarle un módulo de razonamiento lento al lado.
No es añadir una pieza nueva.
No. Sería darle un mejor control sobre su propio zoom. Enriquecer su capacidad para decidir cuándo y cómo su conocimiento debe volverse explícito para ser inspeccionado y corregido. Es un problema de control adaptativo, no de tener dos motores.
O sea que, resumiendo, hemos pasado de ver la mente como una casa con dos motores a verla como un único motor de inferencia muy sofisticado que puede funcionar a diferentes niveles de detalle.
La teoría es, como dice el texto, deflacionaria. Le quita el misterio a la intuición y la superioridad a la deliberación.
Exacto. Disuelve una falsa dicotomía que ha generado muchísima confusión. Y al hacerlo, no es que resuelva el misterio de la mente, sino que nos obliga a hacer preguntas mucho mejores.
Nos enfoca en el problema real.
Nos saca de la discusión inútil de si algo es Sistema 1 o Sistema 2 y nos empuja a investigar el mecanismo real que hay debajo. Y eso nos deja con una pregunta final, que es quizá el verdadero Everest para la ciencia cognitiva y para la IA.
¿Y cuál es esa pregunta?
La pregunta que reemplaza a, ¿tiene Sistema 2? Es mucho más profunda. Es, ¿cómo decide un sistema cognitivo, sea un cerebro o un chip, que su forma habitual de ver el mundo ya no es suficiente para el problema que tiene delante?
¿Cómo sabe que tiene que hacer zoom?
Exacto. Y una vez que toma esa decisión, ¿qué recursos usa para descomprimir y reevaluar? Entender ese mecanismo de control adaptativo. Ese es el verdadero problema que nos queda por resolver.
