WEBVTT

00:00.000 --> 00:06.460
Welcome back to The Deep Dive. Today, we are going to do something a little, a little dangerous.

00:06.660 --> 00:08.080
Oh, I like the sound of that.

00:08.240 --> 00:15.140
We're going to take a concept that I guarantee almost every single person listening right now believes is true.

00:15.580 --> 00:18.180
I mean, something that feels as natural as breathing.

00:18.540 --> 00:18.780
Yeah.

00:19.100 --> 00:22.500
And we are going to try to, well, dismantle it.

00:22.640 --> 00:26.760
It is a bit of a demolition job today, isn't it? But, you know, a necessary one.

00:26.760 --> 00:37.500
Absolutely. Look, if you've been anywhere near a business book, a psychology podcast, or even just like a dinner party in the last 15 years, you know the drill.

00:37.760 --> 00:38.220
Oh, yeah.

00:38.300 --> 00:44.660
You know the Bible of behavioral economics. I'm talking about Daniel Kahneman. I'm talking about thinking fast and slow.

00:44.840 --> 00:50.240
It's the standard model. It's pretty much the framework we all use to talk about thinking.

00:50.540 --> 00:54.220
It's practically the operating system for how we talk about our own minds.

00:54.220 --> 00:59.380
The idea is, on the surface, incredibly simple, right? You have two brains.

00:59.540 --> 01:02.640
Or, to be precise, two systems. System one and system two.

01:02.740 --> 01:03.880
Right. Two systems.

01:04.280 --> 01:16.140
And for most people, that distinction is absolute. System one is the fast one. It's instinct. It's the gut. It's the thing that makes you grab a cookie without thinking.

01:16.280 --> 01:17.160
The impulsive one.

01:17.160 --> 01:24.340
And system two is the slow one. It's logic. It's doing your taxes. It's painful deliberation.

01:24.680 --> 01:36.880
Exactly. It's the angel and the devil on your shoulder. It's the impulsive child and the responsible adult, all wrapped up in one neat package. And we all accept this. I mean, it feels true.

01:36.880 --> 01:48.140
It really does. When I'm driving on an empty highway, just zoned out, I'm in system one. But when I'm trying to calculate a 20% tip at a restaurant while my friends are all watching me.

01:48.180 --> 01:49.320
Oh, the pressure.

01:50.000 --> 01:53.520
That's pure system two. I can feel my brain creaking.

01:53.720 --> 01:59.020
It's a very, very seductive heuristic. It just matches our subjective experience so perfectly.

01:59.020 --> 02:11.580
Well, hold on to your heuristics, because today we're diving into a stack of documents that argues that entire framework is, well, maybe not wrong in its observations, but fundamentally wrong in its conclusions.

02:11.920 --> 02:12.820
This is a big one.

02:13.140 --> 02:22.300
We're looking at a paper titled The Myth of Dual Cognition by an author named Flexion, along with a really spicy explanatory note that came with it.

02:22.300 --> 02:28.940
And the thesis here is radical. It says there are not two systems. There is only one system.

02:29.140 --> 02:36.660
And that one system isn't like switching gears between two different engines. It's simply operating at different levels of resolution.

02:37.320 --> 02:41.480
Resolution. That is the key word we are going to be obsessing over today.

02:41.680 --> 02:47.460
This isn't just semantics, is it? This is not just a nerdy academic dispute about how we label things.

02:47.460 --> 02:51.740
Well, not at all. I mean, if the source material is correct, this changes everything.

02:51.740 --> 02:55.200
It changes how we view our own habits, how we view expertise.

02:55.940 --> 02:57.400
And maybe most importantly.

02:57.440 --> 03:01.840
And perhaps most importantly, it completely reframes the debate around artificial intelligence.

03:02.140 --> 03:05.780
That's the part that really got me. I mean, we're going to get to the AI stuff later.

03:05.920 --> 03:16.020
But the argument here basically says if we misunderstand how we think, we are definitely absolutely misunderstanding how machines think.

03:16.200 --> 03:17.180
That's a huge claim.

03:17.180 --> 03:23.800
We might be accusing AI of being dumb for the exact same reasons that actually make a human smart.

03:24.060 --> 03:30.160
It's a bold claim. But before we get to the robots, we have to deal with the humans.

03:30.760 --> 03:36.060
We have to unpack why we believe in the two system model in the first place. I mean, where did it come from?

03:36.060 --> 03:43.020
Right. Let's start with the status quo, because to understand the disruption, you have to understand what's being disrupted.

03:43.660 --> 03:47.520
When we talk about system one and system two, what are we actually describing?

03:48.060 --> 03:55.800
So the traditional definition, which really solidified with Kahneman and Tversky, posits this dichotomy.

03:56.620 --> 03:57.060
A split.

03:57.600 --> 03:59.420
On one side, you've got system one.

03:59.820 --> 04:00.420
The fast one.

04:00.420 --> 04:08.760
The fast one. It's described as automatic, intuitive, and crucially effortless. It's associative. It just happens.

04:09.060 --> 04:10.340
Give me a concrete example.

04:10.820 --> 04:23.120
Okay. So imagine you see a face. A friend walks into the room and they look absolutely furious. You don't have to break out a calculator. You don't have to measure the angle of their eyebrows or like the redness of their skin. You just know.

04:23.180 --> 04:23.860
It happens to you.

04:23.860 --> 04:31.540
Precisely. You don't do the work. The conclusion just arrives, fully formed. That's the hallmark of system one. It feels involuntary.

04:31.780 --> 04:33.420
Right. It feels like an input.

04:33.620 --> 04:33.780
Yeah.

04:33.840 --> 04:37.220
I didn't do the thinking. The thinking just appeared in my brain.

04:37.320 --> 04:48.800
Exactly. Now compare that to system two. This is slow, deliberative, reflective, and crucially effortful. It consumes resources. It's hard work.

04:48.800 --> 04:52.580
This is my calculating the tip moment. The cold sweat.

04:52.780 --> 04:58.440
Yes. Or if I ask you right now, what's 17 times 24?

04:58.720 --> 05:02.300
Oh, okay. Yeah. I'm stopping everything else. I'd have to like write it down.

05:02.400 --> 05:16.720
Exactly. You stop walking. Literally, studies show your pupils dilate, your blood pressure rises slightly. You have to grind through the steps. You have to hold the numbers in your head, perform an operation, check the result. That is system two.

05:16.720 --> 05:21.060
And it feels physically different. It feels like work. My brain hurts.

05:21.260 --> 05:27.420
It is work. And that's why the distinction feels so real. We have a fast and easy mode and a slow and hard mode. It's intuitive.

05:27.840 --> 05:38.760
But this is where the source material starts to poke holes in the theory. It points out that this didn't start as a claim about brain anatomy or, you know, different modules in the brain.

05:38.860 --> 05:39.720
No, not at all.

05:39.720 --> 05:45.900
It started as a phenomenological observation, which is a fancy way of saying it's just how it feels.

05:46.420 --> 05:58.320
Yes. And that history is vital here. This goes back way before Kahneman. We're talking about William James in the late 19th century. He was talking about the difference between habit and effort.

05:58.320 --> 05:58.980
Right.

05:59.080 --> 06:09.700
He wasn't doing fMRI scans. He had no idea which neurons were firing where. He was just a brilliant observer describing the human experience from the inside out.

06:09.920 --> 06:16.220
So originally, system one and system two were just adjectives. They were just ways of describing a process.

06:16.800 --> 06:25.440
Exactly. There were descriptors. You could say, I'm thinking quickly right now or I'm thinking slowly. I'm thinking effortlessly or I'm thinking effortfully.

06:25.440 --> 06:27.900
It's a description of the texture of thought.

06:28.440 --> 06:34.360
But then something happened. The paper calls it, and I love this term, ontological drift.

06:34.880 --> 06:37.260
Ontological drift. It's such a great phrase.

06:37.720 --> 06:39.880
It sounds like something from a sci-fi movie.

06:40.800 --> 06:46.340
Captain, we're experiencing ontological drift. But what does it actually mean in this context?

06:46.660 --> 06:53.780
Well, it's a philosophical term. Ontology is the study of what exists, what things fundamentally are.

06:53.780 --> 06:56.600
And drift is the error.

06:57.620 --> 07:07.000
So ontological drift happens when we take a descriptive, label-like, fast thinking, and we drift into believing it is a physical thing.

07:07.200 --> 07:09.020
We turn the adjective into a noun.

07:09.260 --> 07:12.840
We turn the adjective into a noun, and then we turn that noun into a machine.

07:13.100 --> 07:19.960
We move from saying, this thought process is fast, to believing there is a fast-thinking machine inside my head.

07:19.960 --> 07:23.960
Okay, I want to use an analogy here to make sure I've got this locked down.

07:24.860 --> 07:28.060
The paper mentions cars, but let's look at a runner.

07:28.200 --> 07:29.600
A runner. Okay, I'm with you.

07:29.760 --> 07:31.080
Say you have an Olympic athlete.

07:31.300 --> 07:31.540
Uh-huh.

07:31.640 --> 07:36.800
Sometimes they sprint. They're going 100% max effort, exploding off the blocks.

07:36.980 --> 07:38.540
That's fast. That's intense.

07:38.800 --> 07:41.820
System two in the old model. All-out effort.

07:42.040 --> 07:45.620
Right. Other times, they're just jogging a warm-up lap.

07:45.960 --> 07:49.140
That's slow. That's easy. They can chat while they do it. System one.

07:49.200 --> 07:50.420
Right. Two different speeds.

07:50.940 --> 07:55.940
Now, ontological drift would be like watching that athlete and saying, oh, I get it.

07:56.140 --> 08:00.200
He has two separate bodies. He has a sprinting body and a jogging body.

08:00.400 --> 08:05.460
And when he wants to go fast, he climbs out of the jogging body and gets into the sprinting body.

08:05.460 --> 08:08.720
That sounds completely absurd when you put it that way.

08:08.720 --> 08:17.100
It is absurd. It's the same legs. It's the same lungs. It's the same heart. It's the same person just operating at a different level of intensity.

08:18.480 --> 08:23.200
But the source material argues that this is exactly what we have done with our brains.

08:23.320 --> 08:24.860
That is a perfect analogy.

08:24.860 --> 08:31.820
We have created a homunculus, a little person inside our heads, or in this case, two little people.

08:32.400 --> 08:43.720
We act as if there is a system one guy who is impulsive and kind of dumb and a system two guy who is smart and rational and they are fighting for control of the steering wheel.

08:44.120 --> 08:46.400
And the source says this isn't just a harmless metaphor.

08:46.760 --> 08:50.600
It leads to what the author calls the moralization of cognition.

08:50.600 --> 09:00.260
And this is a really, really important point. Because once you split the brain into two characters, you inevitably start assigning them moral roles. Good versus evil.

09:00.520 --> 09:02.340
We cast system one as the villain.

09:02.620 --> 09:08.440
We do. In the current worldview, the standard Kahneman worldview system one is the problem.

09:08.840 --> 09:14.960
It's the source of all our biases. It's the thing that makes you racist or sexist or lazy.

09:15.660 --> 09:18.940
It's the thing that eats the donut when you're on a diet.

09:18.940 --> 09:22.760
It's the lizard brain. The primitive part we have to overcome.

09:23.280 --> 09:27.140
And system two is the hero. System two is the adult in the room.

09:27.540 --> 09:30.100
It's rational. It's normative. It's corrective.

09:30.480 --> 09:35.640
We tell ourselves things like, I need to engage my system two to stop doom scrolling.

09:36.080 --> 09:39.940
Or, I need to use logic to overcome my unconscious bias.

09:40.780 --> 09:45.000
It becomes a morality play. The beast versus the angel fighting inside your skull.

09:45.000 --> 09:48.260
Exactly. But the source argues this is a trap.

09:48.820 --> 09:55.860
It suggests that by splitting them into good cop and bad cop, we completely miss the mechanical reality of how thinking actually works.

09:56.140 --> 09:59.400
We are ignoring how the runner actually learns to run in the first place.

09:59.480 --> 10:01.100
So let's get to that mechanical reality.

10:01.240 --> 10:04.800
If there aren't two guys in my head fighting for the steering wheel, what is actually happening?

10:05.140 --> 10:08.160
This brings us to the core thesis of this whole deep dive.

10:08.780 --> 10:10.320
Aspect relegation theory.

10:10.320 --> 10:19.000
Aspect relegation theory. It sounds a bit technical, but I promise you, the concept is beautifully simple once you grasp it.

10:19.080 --> 10:21.160
Okay. Bring it down for us. What's the elevator pitch?

10:21.520 --> 10:24.980
The theory proposes that system one is not a separate thing at all.

10:25.220 --> 10:29.200
It is simply system two processes that have become automated.

10:29.840 --> 10:33.080
Good. Say that again. That's the whole ballgame right there.

10:33.180 --> 10:35.360
System one is just automated system two.

10:35.360 --> 10:38.340
Yes. There is a key quote in the paper I want to read.

10:39.020 --> 10:48.020
System one is the residual form of system two processes that have become automated through repetition, stabilization, and attentional compression.

10:48.300 --> 10:52.760
The residual form. I love that phrasing. It's like what's left over after a process.

10:52.940 --> 10:54.980
I was thinking about it like cooking.

10:55.640 --> 10:58.840
Specifically, think about making a reduction sauce.

10:59.000 --> 11:01.140
Okay. I'm listening. You've got my attention.

11:01.140 --> 11:07.060
You start with a huge pot of liquid stock, wine, herbs, onions, all this stuff.

11:07.260 --> 11:08.780
That is your system two.

11:09.280 --> 11:12.420
It is voluminous. It has all these distinct ingredients.

11:12.600 --> 11:14.520
You can see the individual onions floating in it.

11:14.580 --> 11:16.320
It takes up a lot of space in the pot.

11:16.640 --> 11:17.780
It takes a lot of attention.

11:18.080 --> 11:20.760
That's the high effort phase. The deliberative phase.

11:21.080 --> 11:24.000
Right. Now you simmer it for hours.

11:24.420 --> 11:25.220
You reduce it.

11:25.400 --> 11:28.920
You apply heat and time, which is like practice and repetition.

11:28.920 --> 11:38.140
The water evaporates, the volume shrinks, and eventually you are left with this thick, intense, glossy glaze at the bottom of the pan.

11:38.900 --> 11:40.460
That is system one.

11:40.560 --> 11:41.640
It's the same stuff.

11:41.720 --> 11:43.460
It is chemically the same stuff.

11:43.560 --> 11:48.240
It's the same flavor profile, just unbelievably compressed.

11:48.520 --> 11:50.460
You can't see the individual onions anymore.

11:50.560 --> 11:53.160
They've dissolved into the very essence of the sauce.

11:53.380 --> 11:55.200
But their flavor is still there.

11:55.660 --> 11:57.600
System one is in a different liquid.

11:57.600 --> 12:00.880
It's just the concentrated residue of the original reasoning.

12:01.180 --> 12:01.360
Wow.

12:01.600 --> 12:01.840
Okay.

12:01.940 --> 12:06.820
So my gut feeling isn't some magic bolt of lightning from a different brain.

12:07.080 --> 12:07.320
Yeah.

12:07.560 --> 12:11.020
It's a reduction sauce of all my past experiences and calculations.

12:11.200 --> 12:11.660
Exactly.

12:11.860 --> 12:18.600
And the mechanism for how we get from the big pot of soup to that tiny bit of sauce is what the author calls aspect relegation.

12:19.340 --> 12:20.420
Let's unpack those words.

12:20.820 --> 12:21.600
Aspects and relegation.

12:22.280 --> 12:22.960
What's an aspect?

12:22.960 --> 12:30.180
So any cognitive process, reasoning, driving, doing math, whatever, is made up of aspects.

12:30.740 --> 12:34.240
These are the intermediate steps, the little subroutines, the conditional branches.

12:34.740 --> 12:36.960
If X happens, then I do Y.

12:37.500 --> 12:38.360
The error checks.

12:38.600 --> 12:39.860
Did I carry the one correctly?

12:40.340 --> 12:42.240
Is that light red or is it green?

12:42.580 --> 12:47.200
The little distinct pieces of the thought, the individual instructions in the recipe.

12:47.200 --> 12:47.720
Perfect.

12:48.020 --> 12:48.220
Right.

12:49.020 --> 12:56.120
When a task is new, when you're first making that sauce, you are in what the author calls high resolution.

12:56.980 --> 12:59.300
You have to see every single aspect.

12:59.500 --> 13:02.780
You are monitoring the onions, the heat, the spoon, the timing.

13:03.200 --> 13:05.900
You are consciously aware of every variable.

13:06.120 --> 13:07.740
Because you don't know which ones matter yet.

13:07.820 --> 13:08.460
You're a novice.

13:08.560 --> 13:09.660
You haven't figured out the pattern.

13:09.660 --> 13:10.180
Exactly.

13:10.780 --> 13:17.080
But as you repeat the task, as it becomes stable and predictable, you start to relegate those aspects.

13:17.240 --> 13:18.380
You push them into the background.

13:18.560 --> 13:20.780
You stop paying attention to the intermediate steps.

13:21.020 --> 13:23.600
You just see the input, ingredients, and the output.

13:24.020 --> 13:24.920
Delicious sauce.

13:25.260 --> 13:27.320
The middle part becomes automatic.

13:27.640 --> 13:33.300
This brings us to the central metaphor of the paper, which I think is just brilliant for the digital age.

13:33.640 --> 13:34.960
It's not about engines.

13:35.080 --> 13:36.040
It's about resolution.

13:36.500 --> 13:36.860
Yes.

13:36.860 --> 13:42.520
This moves us away from the two engines idea and toward a one screen idea.

13:42.840 --> 13:44.660
It's a much better way to think about it.

13:44.800 --> 13:47.880
The argument is that the brain isn't switching engines.

13:48.500 --> 13:50.800
It's changing the resolution of its attention.

13:51.260 --> 13:56.480
Think about streaming a video on YouTube or Netflix or whatever you use.

13:56.640 --> 13:58.120
You are watching a movie.

13:58.480 --> 14:02.000
It's the same file, the same plot, same character, same runtime.

14:02.760 --> 14:05.820
That file represents the core reasoning process.

14:05.820 --> 14:09.480
Now, imagine your internet connection is a bit spotty.

14:09.940 --> 14:11.320
You have limited bandwidth.

14:11.880 --> 14:12.720
Limited attention.

14:13.280 --> 14:14.800
Limited cognitive resources.

14:14.960 --> 14:15.380
Exactly.

14:15.600 --> 14:16.720
Bandwidth is attention.

14:16.880 --> 14:20.840
If you have limited bandwidth, the stream automatically drops down to 240p.

14:21.040 --> 14:21.700
It's blocky.

14:21.860 --> 14:22.300
It's blurry.

14:22.300 --> 14:26.140
You can't see the texture of the actor's shirt or the leaves on the trees.

14:26.720 --> 14:29.920
But, and this is key, it loads instantly.

14:30.160 --> 14:32.240
It plays smoothly without buffering.

14:32.400 --> 14:33.360
That is system one.

14:33.880 --> 14:36.100
Low resolution, but high speed and low effort.

14:36.480 --> 14:36.920
Precisely.

14:37.120 --> 14:39.340
But let's say you need to see a specific detail.

14:39.540 --> 14:44.000
There's a clue written on a tiny piece of paper in a movie, and you need to read it to solve the mystery.

14:44.000 --> 14:46.800
You can't read it in 240p, so what do you do?

14:47.120 --> 14:50.780
I pause it, go to the settings, and I crank it up to 4K.

14:51.440 --> 14:54.880
You crank up the resolution, and what happens immediately?

14:54.980 --> 14:55.380
Buffers.

14:55.600 --> 14:57.120
The little spinning wheel of death comes up.

14:57.460 --> 14:58.240
It takes time.

14:58.720 --> 14:59.720
It chews up my data.

14:59.880 --> 15:01.660
It consumes massive bandwidth.

15:02.420 --> 15:03.560
That is system two.

15:03.900 --> 15:05.120
It's not a different movie.

15:05.360 --> 15:11.780
It's the same data, just rendered at a much higher fidelity, because you need to inspect the details, the aspects.

15:11.780 --> 15:14.760
This completely demystifies the experience for me.

15:15.280 --> 15:18.300
When I'm thinking hard, I'm not using a different brain.

15:18.460 --> 15:22.120
I'm just forcing my mental video stream into 4K.

15:22.560 --> 15:24.180
I'm looking at the individual pixels.

15:24.500 --> 15:28.040
And when you are thinking fast, you are accepting the blur.

15:28.600 --> 15:33.440
You are trusting that the general shape of the image is enough to get by for now.

15:33.880 --> 15:34.920
Trusting the blur.

15:35.400 --> 15:36.460
That's a great way to put it.

15:36.460 --> 15:42.520
I want to ground this in a real-world example, because the paper gives a couple of really good ones that make this concrete.

15:43.080 --> 15:44.900
The first one is the commuter.

15:45.160 --> 15:45.620
The commuter.

15:45.800 --> 15:46.080
Yes.

15:46.160 --> 15:51.480
This is a classic example of habit formation, or what this theory would call relegation.

15:51.800 --> 15:52.560
Walk us through it.

15:52.560 --> 15:56.600
Okay, so imagine someone who lives in a big city, like New York or London.

15:57.380 --> 16:01.420
They walk to the subway station every single morning for five years, same route.

16:01.800 --> 16:05.880
They walk out their front door, turn left, cross two streets.

16:06.120 --> 16:10.360
They dodge that same puddle that always forms on the corner after it rains.

16:10.500 --> 16:11.500
We all have that puddle.

16:11.500 --> 16:18.860
They swipe their transit card, walk down the stairs, and stand in the exact same spot on the platform every day.

16:19.300 --> 16:24.720
Now, if you stop them on the platform and ask, how did you get here, describe your walk.

16:24.780 --> 16:25.420
What do they say?

16:25.540 --> 16:27.400
They'll say, I don't know.

16:27.460 --> 16:28.860
I was thinking about dinner tonight.

16:28.920 --> 16:29.960
I was listening to a podcast.

16:30.280 --> 16:31.820
I just, I kind of arrived.

16:32.040 --> 16:33.720
It feels like teleportation.

16:33.800 --> 16:35.260
It feels completely automatic.

16:35.440 --> 16:36.860
It feels like pure system one.

16:37.000 --> 16:37.500
It does.

16:38.000 --> 16:40.720
But let's look at the reality of what they just did.

16:41.520 --> 16:46.640
Navigating a busy city street is an incredibly complex physics and logic problem.

16:47.240 --> 16:51.480
They had to calculate the velocity of oncoming cars to cross the street safely.

16:51.800 --> 16:56.040
They had to navigate complex spatial geometry to turn the corners.

16:56.600 --> 17:00.780
They had to use fine motor skills to swick their car just right.

17:00.780 --> 17:08.760
I mean, if you tried to get a Boston Dynamics robot to do that, it would take millions of lines of code, and it would still probably fall into the puddle.

17:08.880 --> 17:09.300
Exactly.

17:09.300 --> 17:14.900
So, did the commuter use a primitive lizard brain to do all that complex math?

17:15.120 --> 17:15.480
No.

17:16.060 --> 17:22.980
According to this theory, they used the exact same reasoning they used the very first day they moved into that apartment.

17:23.360 --> 17:26.460
But the first day they moved in, it was hard.

17:26.860 --> 17:27.640
It was stressful.

17:27.640 --> 17:29.980
The first day was high resolution.

17:30.200 --> 17:31.020
It was 4K.

17:31.360 --> 17:33.160
Okay, what's the street sign say?

17:33.280 --> 17:34.300
Is this a one-way street?

17:34.500 --> 17:36.000
Okay, there's a puddle on the left side.

17:36.040 --> 17:36.980
I need to remember that.

17:37.280 --> 17:38.680
Where did I put my transit card?

17:39.760 --> 17:42.760
They were streaming every single detail.

17:43.020 --> 17:47.840
But after five years, they have relegated all those aspects.

17:48.020 --> 17:49.020
They've compressed them.

17:49.020 --> 17:50.200
They have solved the problem.

17:50.480 --> 17:53.360
The solution is cached, to use a computer term.

17:53.620 --> 17:58.640
They aren't re-solving the complex problem of how to get to the station every morning.

17:58.880 --> 18:01.540
They are just executing the stored program.

18:01.940 --> 18:05.240
The reasoning, the physics, the geometry is all still there.

18:05.520 --> 18:09.460
It's just been compressed into a smooth 240p stream.

18:09.460 --> 18:14.100
But, and here is the kicker, and this is where the theory really shines for me.

18:14.620 --> 18:19.320
What happens if one day they walk up to the station and the entrance is closed for construction?

18:19.800 --> 18:22.740
A big sign, yellow tape, the words.

18:22.760 --> 18:23.800
Oh yeah, boom.

18:24.260 --> 18:26.480
The automaticity shatters instantly.

18:27.300 --> 18:28.440
Relegation stops.

18:28.540 --> 18:29.920
The blur snaps into focus.

18:30.120 --> 18:32.460
The commuter stops dead in their tracks.

18:32.660 --> 18:33.520
They look around.

18:33.640 --> 18:35.840
The autopilot disengages completely.

18:36.560 --> 18:38.040
Suddenly they are reading signs.

18:38.140 --> 18:39.140
They are checking their watch.

18:39.140 --> 18:41.160
They are pulling out their phone to check the map.

18:41.240 --> 18:42.480
They're looking for a bus stop.

18:42.640 --> 18:44.240
They're calculating a new route.

18:44.380 --> 18:45.760
They are back in 4K.

18:45.880 --> 18:46.120
Yeah.

18:46.420 --> 18:47.600
Full buffering mode.

18:47.720 --> 18:49.580
They are back in what we call system 2.

18:50.220 --> 18:53.540
But notice, they didn't switch brains.

18:53.740 --> 18:57.900
They just re-promoted the aspects of navigation back into conscious attention.

18:58.420 --> 19:00.060
Their brain got an error message.

19:00.440 --> 19:02.000
The cast solution has failed.

19:02.440 --> 19:05.080
I need to open the source file and look at the code again.

19:05.480 --> 19:08.060
That transition is so seamless we don't even notice it.

19:08.060 --> 19:10.680
It's like the auto quality setting on YouTube.

19:11.060 --> 19:12.480
My phone just does it.

19:12.680 --> 19:15.220
It fluctuates based on need, on bandwidth.

19:15.720 --> 19:19.160
And that fluctuation is the essence of intelligence.

19:20.160 --> 19:22.980
It's not about being in system 2 all the time.

19:23.240 --> 19:25.920
That would be completely paralyzing.

19:26.060 --> 19:28.100
And it's not about being in system 1 all the time.

19:28.180 --> 19:29.500
That would be reckless.

19:29.800 --> 19:30.040
Right.

19:30.120 --> 19:32.800
It's about the dynamic regulation of that resolution.

19:33.440 --> 19:36.340
Knowing when to zoom in and when to zoom out.

19:36.340 --> 19:38.300
I want to talk about the driving example, too.

19:38.380 --> 19:40.440
Because I think that connects to the feeling of effort.

19:40.640 --> 19:43.260
We mentioned earlier that system 2 feels like work.

19:43.500 --> 19:43.740
Yes.

19:44.560 --> 19:49.160
Effort is the defining characteristic of the slow system in the old model.

19:49.420 --> 19:53.320
The paper makes a really interesting point about what effort actually is.

19:54.020 --> 19:57.400
Because we tend to equate effort with being smart.

19:58.120 --> 20:01.320
You know, I'm thinking really hard, so I must be being rational.

20:01.320 --> 20:01.720
Right.

20:01.920 --> 20:03.860
We wear effort like a badge of honor.

20:04.440 --> 20:10.440
But the author here argues that effort is simply a bandwidth warning light on your mental dashboard.

20:10.780 --> 20:12.160
A bandwidth warning light.

20:12.440 --> 20:13.120
I like that.

20:13.340 --> 20:14.660
Think about the novice driver.

20:15.020 --> 20:17.460
The 16-year-old with their learner's permit.

20:17.800 --> 20:19.360
I remember when I was learning to drive.

20:19.500 --> 20:21.200
It was physically exhausting.

20:21.860 --> 20:22.360
Oh, totally.

20:22.760 --> 20:27.460
I remember gripping the steering wheel so hard my knuckles were white I couldn't have the radio on.

20:27.460 --> 20:30.120
My mom tried to talk to me from the passenger seat.

20:30.200 --> 20:30.900
I'd snap at her.

20:31.180 --> 20:31.540
Quiet.

20:31.800 --> 20:32.360
I'm urging.

20:33.000 --> 20:33.440
Exactly.

20:34.040 --> 20:36.020
You were in maximum high resolution.

20:36.340 --> 20:39.060
You were tracking 50 different variables explicitly.

20:39.340 --> 20:41.760
The pressure of your foot on the gas pedal.

20:42.180 --> 20:44.560
The exact angle of the rearview mirror.

20:44.900 --> 20:46.460
The distance to the curb.

20:46.620 --> 20:48.620
The speed of the car behind you.

20:49.000 --> 20:53.240
You were holding all these aspects in your active working memory.

20:53.440 --> 20:54.240
Your RAM.

20:54.580 --> 20:55.920
And that burns energy.

20:55.920 --> 20:57.520
That is the effort.

20:57.980 --> 21:02.060
That's the feeling of my brain's CPU running at 100%.

21:02.060 --> 21:03.060
That is the effort.

21:03.240 --> 21:05.960
You are keeping all those plates spinning manually.

21:06.320 --> 21:07.300
Now, look at you today.

21:07.500 --> 21:09.080
You probably drive to work.

21:09.160 --> 21:10.000
You're drinking coffee.

21:10.240 --> 21:11.480
You're listening to this podcast.

21:11.780 --> 21:13.280
You're arguing with the radio host.

21:13.460 --> 21:15.380
You are barely looking at the road.

21:15.640 --> 21:17.320
Hopefully looking at the road a little bit.

21:17.460 --> 21:18.340
For legal reasons.

21:18.720 --> 21:19.740
Well, yes.

21:20.360 --> 21:23.260
But you aren't consciously thinking about the pedal pressure.

21:23.260 --> 21:26.340
You aren't calculating the angle of the turn.

21:26.780 --> 21:28.520
You have relegated those aspects.

21:28.980 --> 21:29.920
They have been compressed.

21:30.460 --> 21:34.520
The reasoning structure, how to drive a car, hasn't vanished.

21:34.760 --> 21:36.340
You still know how to drive.

21:36.460 --> 21:38.600
In fact, you know it better than the 16-year-old.

21:38.900 --> 21:41.960
But because I know it better, it feels like I'm doing less.

21:42.040 --> 21:42.400
It feels effortless.

21:42.400 --> 21:42.960
Effortless.

21:43.120 --> 21:43.600
Exactly.

21:44.220 --> 21:45.540
And this is the paradox.

21:46.200 --> 21:49.940
As you become an expert, the feeling of effort decreases.

21:50.480 --> 21:54.080
You move from what feels like System 2 to what feels like System 1.

21:54.460 --> 22:02.280
So if we follow the old model, we'd have to say the expert driver is thinking less or being less rational than the novice.

22:02.580 --> 22:03.840
Which is just ridiculous.

22:04.100 --> 22:05.900
The expert is obviously a better driver.

22:05.980 --> 22:06.300
Right.

22:06.600 --> 22:08.600
The expert has simply compiled the code.

22:08.600 --> 22:13.180
The novice is running the code line by line, interpreting it as they go in real time.

22:13.320 --> 22:14.480
That's slow and hard.

22:14.900 --> 22:17.640
The expert is running the compiled, executable file.

22:18.100 --> 22:20.700
It runs instantly and silently in the background.

22:21.280 --> 22:26.240
This leads us naturally into the most mystical, most romanticized part of human cognition.

22:27.180 --> 22:27.700
Intuition.

22:27.920 --> 22:28.840
Ah, yes.

22:29.380 --> 22:30.760
The magic of the gut feeling.

22:31.120 --> 22:32.580
We love to talk about intuition.

22:33.440 --> 22:35.960
Especially in business or creative fields.

22:35.960 --> 22:38.920
We idolize the CEO who says,

22:39.140 --> 22:40.880
I didn't look at the data.

22:41.380 --> 22:42.640
I just went with my gut.

22:43.480 --> 22:45.060
We treat it like a superpower.

22:45.360 --> 22:48.440
Like it's a direct line to some cosmic truth.

22:48.540 --> 22:51.380
We treat it as if it is fundamentally distinct from reasoning.

22:51.840 --> 22:52.980
You know, don't overthink it.

22:53.040 --> 22:55.100
Just feel it as if they're opposites.

22:55.380 --> 22:59.140
But Flixin, the author here, is basically the party pooper.

22:59.300 --> 23:01.760
He's the magician revealing how the trick is done.

23:02.180 --> 23:04.500
What is the definition of intuition in this theory?

23:04.500 --> 23:06.840
It is remarkably unmagical.

23:07.060 --> 23:08.920
And I think that's why it's so powerful.

23:09.180 --> 23:15.900
The paper defines intuition as successful concealment of previously explicit inferential structure.

23:16.160 --> 23:17.020
Successful concealment.

23:17.100 --> 23:18.840
That is so dry.

23:19.040 --> 23:19.600
So clinical.

23:19.780 --> 23:21.280
It is dry, but it's so accurate.

23:21.380 --> 23:27.200
It basically says intuition is just reasoning that has been compressed so tightly that you can no longer see the steps.

23:27.280 --> 23:28.580
You've hidden the work from yourself.

23:28.760 --> 23:32.560
There's a phrase in the source that I highlighted three times because it's so good.

23:32.560 --> 23:35.740
Intuition is just yesterday's reasoning.

23:36.420 --> 23:37.340
Efficiently forgotten.

23:38.020 --> 23:39.040
Yesterday's reasoning.

23:39.520 --> 23:40.400
Efficiently forgotten.

23:41.220 --> 23:42.000
I love that.

23:42.080 --> 23:42.260
Yeah.

23:42.500 --> 23:45.320
It's what you'd call a deflationary view of intuition.

23:45.740 --> 23:47.800
Takes all the hot air out of the balloon.

23:48.060 --> 23:49.840
So let's apply this.

23:49.980 --> 23:50.140
Yeah.

23:50.220 --> 23:51.640
Say I'm a seasoned detective.

23:51.900 --> 23:53.460
I've been on the force for 30 years.

23:53.460 --> 23:55.520
I walk into a crime scene.

23:55.640 --> 23:57.620
Within five seconds, I look around.

23:57.720 --> 23:59.380
I say, the husband did it.

23:59.560 --> 23:59.680
Right.

24:00.160 --> 24:02.900
In the movies, that's presented as a psychic flash.

24:03.160 --> 24:06.180
A moment of pure, unexplainable genius.

24:06.600 --> 24:10.240
But under aspect relegation theory, what just happened in my brain?

24:10.360 --> 24:14.140
What happened is that your brain scanned the room in high speed, low resolution.

24:14.800 --> 24:20.020
You noticed, without consciously realizing it, that there was no sign of forced entry.

24:20.020 --> 24:24.260
You noticed the husband's body language was defensive, not grieving.

24:24.560 --> 24:28.600
You noticed a glass on the table was wiped clean when everything else was dusty.

24:28.840 --> 24:32.380
I saw the onions in the reduction sauce, but I just tasted the final sauce.

24:32.480 --> 24:33.000
Exactly.

24:33.200 --> 24:35.360
You processed a dozen clues in parallel.

24:36.100 --> 24:40.700
Clues that 20 years ago, as a rookie detective, you would have had to write down in a notebook

24:40.700 --> 24:42.460
and stare at for hours.

24:42.740 --> 24:42.900
Okay.

24:43.100 --> 24:43.700
Clue one.

24:43.960 --> 24:44.980
No forced entry.

24:45.260 --> 24:45.840
Clue two.

24:47.180 --> 24:48.640
Weird body language.

24:48.640 --> 24:50.240
You would have had to do it in 4K.

24:50.820 --> 24:51.160
Yes.

24:51.620 --> 24:57.480
But because you've done this a thousand times, your brain ran the crime scene analysis script

24:57.480 --> 25:01.460
in the background and just handed you the output, the final answer.

25:02.120 --> 25:02.620
Husband.

25:03.800 --> 25:10.400
So the gut feeling is actually just a hyper-fast summary of evidence presented to your consciousness

25:10.400 --> 25:11.200
as a feeling.

25:11.500 --> 25:11.940
Correct.

25:11.940 --> 25:17.900
It feels like the answer just appears only because the intermediate work is hidden from

25:17.900 --> 25:18.820
your conscious view.

25:19.060 --> 25:20.160
You don't see the factory.

25:20.580 --> 25:23.180
You just get the finished product delivered to your doorstep.

25:23.660 --> 25:29.060
This really explains why trusting your gut is only good advice if you're actually an expert

25:29.060 --> 25:29.880
in that domain.

25:30.120 --> 25:30.700
Oh, absolutely.

25:30.920 --> 25:32.340
That is the critical takeaway.

25:33.020 --> 25:37.280
If a novice in a field tries to trust their gut, they are just guessing.

25:37.680 --> 25:39.440
They are just expressing their biases.

25:39.440 --> 25:41.520
They don't have the compressed reasoning.

25:41.840 --> 25:43.600
They don't have the reduction sauce.

25:43.760 --> 25:45.300
All they have is hot water.

25:45.440 --> 25:46.520
There's nothing to reduce.

25:46.740 --> 25:47.280
Perfectly put.

25:47.380 --> 25:49.080
You have to do the work in system two.

25:49.320 --> 25:54.120
You have to do the slow, deliberative learning before you can earn the speed of system one.

25:54.280 --> 25:57.620
You can't have the glaze without the long, slow reduction.

25:58.020 --> 25:58.260
Okay.

25:58.360 --> 26:00.020
So we've dismantled the human mind.

26:00.400 --> 26:05.980
We've turned intuition into zip files and expert driving into compiled code.

26:05.980 --> 26:08.560
But the source material doesn't stop there.

26:09.160 --> 26:13.600
It takes this theory and throws a grenade right into the middle of the biggest debate

26:13.600 --> 26:14.880
in technology right now.

26:15.020 --> 26:16.180
Artificial intelligence.

26:16.440 --> 26:18.440
Specifically large language models.

26:18.920 --> 26:21.420
Chachi-BT, Claude, Gemini.

26:22.160 --> 26:24.820
The AIs that we are all talking about constantly.

26:25.040 --> 26:29.940
This is where the paper gets a little combative, especially in that explanatory note.

26:30.340 --> 26:34.120
It explicitly calls out the Gary Marcus style of critique.

26:34.120 --> 26:37.480
We need to set the stage here for anyone who might not be familiar.

26:37.780 --> 26:43.260
Who is Gary Marcus and what is the standard critique of these AI models?

26:43.400 --> 26:47.460
Because I hear this argument all the time, even from people who don't know who Gary Marcus is.

26:47.540 --> 26:54.420
So Gary Marcus is a cognitive scientist and a very vocal, very prominent critic of the current AI hype.

26:54.820 --> 27:01.680
His argument, and I'm simplifying a bit here, but not much, is that these models are just stochastic parrots.

27:01.680 --> 27:03.680
Stochastic parrots.

27:03.880 --> 27:05.020
It's such a great insult.

27:05.200 --> 27:05.920
So sticky.

27:06.340 --> 27:07.580
It sticks, doesn't it?

27:07.720 --> 27:10.180
Stochastic just means random or probabilistic.

27:10.820 --> 27:12.260
The idea is this.

27:13.180 --> 27:16.240
A parrot can learn to say, Polly wants a cracker.

27:16.880 --> 27:21.420
It can make the sounds perfectly, but the parrot doesn't know what a cracker is.

27:21.820 --> 27:23.280
It doesn't understand hunger.

27:23.520 --> 27:25.620
It doesn't understand the concept of wanting.

27:25.620 --> 27:27.340
It's just mimicry.

27:27.660 --> 27:28.900
It's not understanding.

27:29.180 --> 27:31.280
It's just associating sounds.

27:31.660 --> 27:31.780
Right.

27:32.080 --> 27:36.680
And the critique says that LLMs are doing the exact same thing on a massive scale.

27:37.020 --> 27:42.920
They have ingested the entire internet and they are just predicting the next word based on probability.

27:43.320 --> 27:47.840
They see the cat sat on them and they know Matt is a very likely next word.

27:47.840 --> 27:55.240
So they have system one, this ability to associate patterns really, really fast, but they lack system two.

27:55.360 --> 27:56.080
They can't reason.

27:56.240 --> 27:57.300
They can't stop and think.

27:57.480 --> 27:59.800
They were just autocomplete on steroids.

27:59.940 --> 28:00.680
That's the argument.

28:00.840 --> 28:02.100
I'm sure you've heard it a hundred times.

28:02.200 --> 28:03.280
It's not real intelligence.

28:03.500 --> 28:04.500
It's just statistics.

28:04.860 --> 28:06.840
It's a pattern matcher, not a thinker.

28:06.960 --> 28:10.020
But here comes Flitch in with aspect relegation theory.

28:10.140 --> 28:12.300
And he says, wait a minute.

28:12.980 --> 28:15.780
You are making a fundamental category error.

28:15.840 --> 28:17.100
A category error.

28:17.100 --> 28:17.900
Unpack that.

28:18.020 --> 28:19.200
Why is it an error?

28:19.740 --> 28:21.720
Well, think about what we just learned about humans.

28:22.400 --> 28:36.960
If system one is just compiled system two, if intuition is just reasoning that has been stabilized and compressed, then accusing AI of only having system one is a completely meaningless statement.

28:37.200 --> 28:41.020
Because you're basically accusing the AI of acting like a human expert.

28:41.260 --> 28:41.780
Exactly.

28:41.780 --> 28:47.420
If I watch a chess grandmaster play a game of speed chess, they are moving instantly.

28:47.880 --> 28:50.700
They aren't pausing for minutes to calculate every move.

28:50.820 --> 28:52.540
They are playing on intuition.

28:52.740 --> 28:54.940
They are using what feels like system one.

28:55.180 --> 29:01.720
If I told you that grandmaster isn't actually thinking, he's just pattern matching, you'd laugh at me.

29:01.720 --> 29:07.640
I'd say he's pattern matching because he has mastered the logic of chess over tens of thousands of hours.

29:08.060 --> 29:09.780
His pattern matching is his thinking.

29:10.100 --> 29:10.280
Right.

29:10.480 --> 29:14.000
His speed is proof of his competence, not proof of his stupidity.

29:14.360 --> 29:22.020
The source argues that when we see ChatGPT spit out a complex coding solution in three seconds, our reaction shouldn't be, it's not thinking.

29:22.020 --> 29:26.220
It should be, it is thinking at an incredibly high level of compression.

29:26.840 --> 29:33.360
So just because I can't see the AI stopping to think doesn't mean the logic isn't there embedded in the system.

29:33.560 --> 29:37.260
The author uses a programming analogy that is really, really helpful here.

29:37.780 --> 29:40.780
Think about source code versus machine code.

29:40.920 --> 29:41.120
Okay.

29:41.480 --> 29:42.820
Source code is what humans write.

29:43.200 --> 29:45.300
It's in Python or C plus tells me.

29:45.400 --> 29:45.620
Right.

29:45.740 --> 29:47.940
If you write a program in Python, it's readable.

29:47.940 --> 29:51.160
It says, if X is greater than five, then print the word hello.

29:51.900 --> 29:53.960
You can see the logical steps clearly.

29:54.820 --> 29:56.080
That is system two.

29:56.940 --> 29:58.020
It's explicit.

29:58.380 --> 29:59.300
It's slow to write.

29:59.480 --> 30:00.580
It's easy to understand.

30:01.180 --> 30:01.880
And machine code.

30:02.020 --> 30:02.360
What's that?

30:02.700 --> 30:08.220
Machine code is what happens after you compile the program so the computer can actually run it.

30:08.380 --> 30:12.180
It turns into a massive string of binary, zeros and ones.

30:12.440 --> 30:14.540
It's completely unreadable to a human.

30:14.760 --> 30:16.460
The if then statements are gone.

30:16.460 --> 30:20.260
The logic is like smeared across the entire file.

30:20.420 --> 30:22.440
It just executes instantly.

30:22.680 --> 30:25.000
But the logic is still inside the zeros and ones.

30:25.000 --> 30:27.720
It has to be or the program wouldn't work.

30:28.080 --> 30:28.680
Precisely.

30:29.160 --> 30:36.680
The author argues that criticizing AI for only having system one is like looking at the compiled machine code and saying,

30:36.980 --> 30:40.360
this isn't real computation because I can't read the source code anymore.

30:40.360 --> 30:48.900
The reasoning steps are hidden in the weights of the neural network, much like they are hidden in the expert driver's brain or the detective's gut.

30:48.900 --> 30:52.200
That is, wow, that really flips the script.

30:52.780 --> 30:58.840
It suggests that the stochastic parrot argument is actually punishing AI for being too efficient?

30:59.200 --> 31:02.640
It suggests we are mistaking fluency for lack of thought.

31:02.860 --> 31:06.680
We're mistaking the compiled executable for a dumb parrot.

31:06.680 --> 31:12.080
So does this mean the author thinks AI is perfect, that there's no problem here?

31:12.140 --> 31:22.120
Because I have definitely seen these models make some pretty stupid non-human mistakes, hallucinations, making up facts, getting basic logic wrong.

31:22.120 --> 31:24.200
And the source material absolutely admits that.

31:24.340 --> 31:25.840
It's not an AI hype piece.

31:25.980 --> 31:27.360
It's not saying AI is perfect.

31:27.480 --> 31:29.820
It's saying we are diagnosing the problem wrong.

31:30.180 --> 31:32.220
The problem isn't a lack of system two.

31:32.780 --> 31:36.320
The problem is a lack of dynamic regulation of resolution.

31:36.680 --> 31:39.100
Okay, bring that back to the commuter example for me.

31:39.340 --> 31:40.840
Think about our commuter again.

31:41.520 --> 31:45.980
When the subway entrance is closed, the commuter snaps out of it.

31:45.980 --> 31:51.180
They have an internal trigger, a feeling of surprise, of confusion that says,

31:51.560 --> 31:54.120
Hey, the autopilot is failing.

31:54.640 --> 31:57.400
Promote these aspects to system two immediately.

31:57.540 --> 31:58.220
Open the aperture.

31:58.580 --> 32:00.760
We need 4K resolution right now.

32:01.220 --> 32:03.280
The 240p stream is not working.

32:03.720 --> 32:04.100
Correct.

32:04.640 --> 32:07.260
Humans have what the author calls endogenous control.

32:07.680 --> 32:09.600
We can self-regulate our resolution.

32:09.880 --> 32:11.200
We know when we are confused.

32:11.600 --> 32:14.480
We feel it when a situation is weird or unexpected.

32:14.480 --> 32:16.340
And AI doesn't have that feeling.

32:17.280 --> 32:18.500
Currently, not really.

32:18.700 --> 32:20.260
AI is kind of stuck in one gear.

32:20.720 --> 32:24.760
It generates the answer to a complex physics riddle with the same resolution

32:24.760 --> 32:28.680
and the same level of confidence that it generates a poem about a cat

32:28.680 --> 32:30.420
or a simple coding solution.

32:30.720 --> 32:32.040
It doesn't have that internal,

32:32.500 --> 32:33.480
Wait a minute, this is tricky.

32:33.640 --> 32:37.060
I need to slow down and unpack my compressed reasoning trigger.

32:37.500 --> 32:41.720
So it's like a driver who is on autopilot and keeps driving at 60 miles per hour

32:41.720 --> 32:43.200
even though the bridge ahead is out

32:43.200 --> 32:45.640
because it doesn't have the internal mechanism to say

32:45.640 --> 32:48.340
situation changed, disengage autopilot.

32:48.900 --> 32:49.980
That is the perfect analogy.

32:50.180 --> 32:52.220
It lacks the metacognitive layer.

32:52.380 --> 32:53.600
It doesn't know what it doesn't know.

32:53.900 --> 32:55.740
It doesn't know when to widen the aperture.

32:56.060 --> 32:58.200
It's not that it can't reason in 4K.

32:58.420 --> 33:01.640
It's that it doesn't know when to stop streaming in 240p.

33:01.840 --> 33:03.500
That is such a crucial distinction.

33:04.020 --> 33:08.000
It completely shifts the goalpost for AI research, doesn't it?

33:08.000 --> 33:13.940
We don't need to build a logic module and try to bolt it onto the language module.

33:14.300 --> 33:16.620
We need to teach the system introspection.

33:17.060 --> 33:17.340
Yes.

33:18.020 --> 33:20.560
We need to teach it to recognize its own uncertainty.

33:21.020 --> 33:25.600
We need systems that can say, I don't know, or let me think about that,

33:25.700 --> 33:28.960
or let me expand my search and double check my work.

33:28.960 --> 33:32.640
The next breakthrough in AI won't be about getting faster.

33:33.180 --> 33:35.940
It will be about learning how and when to slow down.

33:36.300 --> 33:37.340
The ability to slow down.

33:37.420 --> 33:37.960
That's poetic.

33:38.280 --> 33:38.720
It is.

33:38.820 --> 33:44.860
It's about restoring the ability to access the source code when the compiled code fails you.

33:45.020 --> 33:46.920
I want to play devil's advocate for a second here

33:46.920 --> 33:50.260
because the source material anticipates some objections,

33:50.640 --> 33:53.320
and I think the listeners might be screaming at their advices right now

33:53.320 --> 33:55.000
about one specific thing.

33:55.060 --> 33:55.820
Lay it on me.

33:55.880 --> 33:56.360
I'm ready.

33:56.360 --> 34:01.580
We are saying all this system one stuff is just learned expertise.

34:02.160 --> 34:04.280
It's just relegated system two.

34:04.880 --> 34:06.380
But what about reflexes?

34:06.820 --> 34:09.980
If I hear a loud bang right behind me, I duck.

34:10.440 --> 34:13.500
If I touch a hot stove, I pull my hand away instantly.

34:14.180 --> 34:15.180
I didn't learn that.

34:15.340 --> 34:18.680
I didn't sit in a classroom and study loud bang theory.

34:18.860 --> 34:19.640
Loud bang theory?

34:19.740 --> 34:21.100
No, you definitely didn't.

34:21.180 --> 34:22.340
That wasn't compressed reasoning.

34:22.440 --> 34:23.460
That was just hardwired.

34:23.640 --> 34:25.540
I was born with that program.

34:25.540 --> 34:30.660
So doesn't that prove there is a separate fast brain, a lizard brain?

34:30.840 --> 34:34.780
This is the innate automaticity objection, and you are absolutely right.

34:34.860 --> 34:36.140
The source acknowledges this.

34:36.380 --> 34:38.820
There is such a thing as hardwired automaticity.

34:39.060 --> 34:40.820
Evolution gave you reflexes.

34:40.980 --> 34:42.460
That was never a system two.

34:42.760 --> 34:44.620
It's not a reduction sauce you cooked up.

34:44.620 --> 34:47.620
It's just raw ingredients you were born with.

34:48.100 --> 34:49.220
So doesn't that break the theory?

34:49.420 --> 34:52.360
Doesn't that prove there are two systems, one learned and one innate?

34:52.580 --> 34:56.640
The author argues, no, it doesn't break the theory for the purposes of this discussion.

34:56.900 --> 34:57.680
And here's why.

34:58.180 --> 35:03.120
The things we actually care about when we talk about intelligence, math, language, logic,

35:03.320 --> 35:09.160
strategy, chess, driving a car, diagnosing a disease, all of that stuff is relegated automaticity.

35:09.160 --> 35:10.360
Ah, I see.

35:10.760 --> 35:12.580
No one is born playing chess.

35:12.940 --> 35:15.920
No one is born speaking French fluently.

35:16.080 --> 35:16.600
Exactly.

35:16.900 --> 35:21.440
No one is born knowing how to navigate a subway system or write a podcast script.

35:21.620 --> 35:27.160
All the stuff that makes us smart, all the complex cognitive stuff we're trying to replicate

35:27.160 --> 35:32.500
in AI was learned through a process of moving from high resolution to low resolution.

35:33.240 --> 35:37.340
So for the context of intelligence and AI, the hardwired stuff is less relevant.

35:37.720 --> 35:39.960
The debate is about the learned capabilities.

35:40.420 --> 35:40.740
Got it.

35:40.960 --> 35:47.280
So we're distinguishing between biological reflexes, which are hardware, and cognitive intuition,

35:47.560 --> 35:49.940
which is software we wrote ourselves over time.

35:50.140 --> 35:50.660
Precisely.

35:50.900 --> 35:53.620
The theory of aspect relegation applies to the software.

35:53.620 --> 35:57.360
Okay, there's another objection here about metacognition, which we touched on.

35:57.500 --> 35:58.940
The idea of waking up.

35:59.240 --> 36:00.080
How do we do that?

36:00.160 --> 36:00.940
What does that trigger?

36:01.200 --> 36:02.140
This is the big mystery.

36:02.400 --> 36:07.300
The paper identifies this as the open problem for cognitive science and for AI.

36:08.240 --> 36:11.000
Humans have this incredibly rich set of triggers.

36:12.080 --> 36:16.100
Anxiety, surprise, cognitive dissonance.

36:16.260 --> 36:20.560
That feeling you get in your stomach when something just isn't right, when the story doesn't add up.

36:20.780 --> 36:21.040
Right.

36:21.040 --> 36:27.860
When you are driving and it suddenly starts raining really hard, you feel a physical tension.

36:28.160 --> 36:29.820
That effort signal kicks in.

36:29.980 --> 36:34.600
It forces you to grip the wheel, to pay more attention, to wake up from autopilot.

36:34.960 --> 36:39.660
And an AI, well, an AI doesn't have anxiety, doesn't get that stomach feeling.

36:39.800 --> 36:40.400
Not yet.

36:40.740 --> 36:48.120
The paper suggests that feeling effort or feeling confusion is the very mechanism that regulates our mental resolution.

36:48.120 --> 36:54.160
And since AI doesn't feel, in a biological sense, it doesn't know when to switch modes.

36:54.640 --> 36:58.900
So maybe the future of AI is we need to give it anxiety.

36:59.360 --> 37:00.760
That sounds like a terrible idea.

37:01.080 --> 37:02.520
In a functional sense, maybe.

37:02.640 --> 37:09.620
It needs an internal signal that says my current model of the world isn't matching the data coming in, error wake up, increase resolution.

37:09.620 --> 37:12.700
That is both terrifying and kind of amazing.

37:13.020 --> 37:19.680
The idea that Marvin the paranoid android from Hitchhiker's Guide might actually be the peak of AI evolution.

37:19.740 --> 37:24.560
It might be a necessary component for true, robust intelligence, a little bit of self-doubt.

37:24.780 --> 37:26.340
Okay, so let's bring this all home.

37:26.760 --> 37:29.200
What does this all mean for us, for the listener?

37:29.200 --> 37:32.140
We've dismantled the two-brain theory.

37:32.600 --> 37:37.000
We've realized our intuition is just zipped files of our own past work.

37:37.400 --> 37:39.880
We've realized AI might need anxiety.

37:40.560 --> 37:43.280
How does this change how I go about my Tuesday?

37:43.680 --> 37:45.300
I think there are two big practical takeaways.

37:45.520 --> 37:49.860
The first is about re-evaluating what we call lazy thinking.

37:49.860 --> 37:53.240
We are so hard on ourselves for being on autopilot.

37:53.380 --> 38:00.820
We read all these mindfulness books, and we think we should be present and logical and in system, too, 100% of the time.

38:00.960 --> 38:01.400
We do.

38:01.880 --> 38:05.220
But the source argues this is just cognitive economy.

38:05.560 --> 38:06.740
It is adaptive.

38:07.100 --> 38:08.080
It is smart.

38:08.260 --> 38:15.960
If you tried to walk to the subway in high-resolution mode every day, thinking about every muscle movement, every single paver on the sidewalk,

38:15.960 --> 38:20.000
you would collapse from exhaustion before you even got to the corner.

38:20.160 --> 38:21.740
You would completely crash the system.

38:21.860 --> 38:22.940
You have to relegate.

38:23.140 --> 38:23.840
You have to compress.

38:24.720 --> 38:32.620
Being on autopilot for the routine stuff is what allows you to have the bandwidth, the 4K streaming capability for the important stuff.

38:33.140 --> 38:35.740
You shouldn't try to be in system two all the time.

38:35.860 --> 38:36.860
That's not being smart.

38:37.000 --> 38:39.080
That's being profoundly inefficient.

38:39.600 --> 38:41.000
So embrace the blur.

38:41.540 --> 38:45.140
It's okay to watch the movie in 240p if it's a boring scene.

38:45.400 --> 38:45.880
Exactly.

38:46.480 --> 38:48.120
Save the 4K for the climax.

38:48.840 --> 38:52.240
Save your mental bandwidth for the decisions that actually matter.

38:52.240 --> 38:53.740
And the second takeaway.

38:54.120 --> 38:54.580
What's that?

38:55.120 --> 38:58.040
It's about moving away from this binary thinking.

38:58.260 --> 39:02.880
Stop thinking fast versus slow or logic versus emotion.

39:03.060 --> 39:03.400
Yes.

39:04.000 --> 39:08.220
Stop asking, am I being emotional or am I being rational right now?

39:08.720 --> 39:12.000
Start asking, what resolution does this problem require?

39:12.580 --> 39:13.380
It's a continuum.

39:13.700 --> 39:14.840
It's a sliding scale.

39:14.840 --> 39:16.760
It's a dial, not a switch.

39:17.000 --> 39:18.940
That is so much more helpful.

39:19.340 --> 39:21.120
Sometimes I feel like I need to be logical.

39:21.120 --> 39:29.440
But maybe I just need to turn up the dial a little bit, zoom in on one or two details, not try to flip a switch to a different brain.

39:29.440 --> 39:31.080
And it helps with learning, too.

39:31.080 --> 39:35.680
When you are learning something new, you have to accept that you must be in high resolution.

39:35.880 --> 39:36.680
It will feel hard.

39:36.820 --> 39:37.640
It will feel slow.

39:37.800 --> 39:39.520
It will feel deeply uncomfortable.

39:39.840 --> 39:40.720
That's the grind.

39:41.200 --> 39:42.900
That feeling of incompetence.

39:42.900 --> 39:44.280
That's the grind.

39:44.280 --> 39:47.660
But knowing this theory, you can reframe it.

39:48.080 --> 39:48.920
I'm not dumb.

39:49.180 --> 39:50.760
I'm just writing the source code.

39:51.920 --> 39:54.860
Eventually, with enough practice, it will compile.

39:55.560 --> 39:57.520
Eventually, it will become effortless.

39:57.520 --> 40:00.240
But you can't skip the coding phase.

40:00.760 --> 40:03.280
You can't just download the compiled file.

40:03.440 --> 40:05.960
You can't have the intuition without putting in the work first.

40:06.120 --> 40:09.500
You can't have the reduction sauce without the long simmer.

40:10.140 --> 40:10.620
Exactly.

40:11.080 --> 40:17.640
As we wrap up, I want to leave the listeners with a final thought from the source material that really, really twisted my brain.

40:18.480 --> 40:21.080
It's about the nature of consciousness itself.

40:21.220 --> 40:22.020
Oh, yes.

40:22.540 --> 40:24.180
This is the provocative end note.

40:24.300 --> 40:26.740
This is the real philosophical twist at the end.

40:26.740 --> 40:37.380
The source suggests that system two, that conscious, deliberative, hard thinking that we prize so much, the thing we think makes us human, isn't actually the superior mode of thinking.

40:37.580 --> 40:38.540
It's not the goal.

40:38.820 --> 40:41.740
It argues that consciousness is just the training mode.

40:42.060 --> 40:48.080
It implies that conscious thought is the clumsy, inefficient state of not having mastered a subject yet.

40:48.420 --> 40:49.660
Just think about that for a second.

40:49.660 --> 41:00.120
When you are perfect at something like walking or speaking your native language or a musician playing a song they've practiced 10,000 times, you are unconscious of it.

41:00.620 --> 41:01.320
You just do it.

41:01.320 --> 41:08.820
It's only when you are bad at it or learning it or fixing a mistake that you have to become conscious and deliberate.

41:09.540 --> 41:22.220
So if we follow that logic to its absolute limit, a perfect mind, a mind that had mastered everything, a godlike intelligence would be entirely unconscious.

41:22.220 --> 41:31.520
It would be pure automaticity, pure, effortless, relegated system one, no inner monologue, no feeling of effort.

41:31.860 --> 41:36.760
It completely questions our entire assumption that consciousness is the pinnacle of evolution.

41:37.520 --> 41:45.240
Maybe consciousness is just the scaffolding we use to build our habits and our expertise, and once the building is done, we are supposed to take the scaffolding down.

41:45.460 --> 41:48.620
Maybe the goal of all our thinking is to finally be able to stop thinking.

41:48.620 --> 41:53.620
That is a thought worth meditating on, or perhaps not meditating on.

41:53.680 --> 42:00.280
Well, on that absolute existential cliffhanger, I'm going to go relegate some aspects of my commute home.

42:00.540 --> 42:02.100
I'm going to try to enjoy the blur.

42:02.360 --> 42:03.460
Try not to overthink it.

42:03.720 --> 42:05.440
Thanks for listening to The Deep Dive.

42:05.580 --> 42:06.500
We'll catch you next time.

