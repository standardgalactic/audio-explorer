1
00:00:00,000 --> 00:00:06,460
Welcome back to The Deep Dive. Today, we are going to do something a little, a little dangerous.

2
00:00:06,660 --> 00:00:08,080
Oh, I like the sound of that.

3
00:00:08,240 --> 00:00:15,140
We're going to take a concept that I guarantee almost every single person listening right now believes is true.

4
00:00:15,580 --> 00:00:18,180
I mean, something that feels as natural as breathing.

5
00:00:18,540 --> 00:00:18,780
Yeah.

6
00:00:19,100 --> 00:00:22,500
And we are going to try to, well, dismantle it.

7
00:00:22,640 --> 00:00:26,760
It is a bit of a demolition job today, isn't it? But, you know, a necessary one.

8
00:00:26,760 --> 00:00:37,500
Absolutely. Look, if you've been anywhere near a business book, a psychology podcast, or even just like a dinner party in the last 15 years, you know the drill.

9
00:00:37,760 --> 00:00:38,220
Oh, yeah.

10
00:00:38,300 --> 00:00:44,660
You know the Bible of behavioral economics. I'm talking about Daniel Kahneman. I'm talking about thinking fast and slow.

11
00:00:44,840 --> 00:00:50,240
It's the standard model. It's pretty much the framework we all use to talk about thinking.

12
00:00:50,540 --> 00:00:54,220
It's practically the operating system for how we talk about our own minds.

13
00:00:54,220 --> 00:00:59,380
The idea is, on the surface, incredibly simple, right? You have two brains.

14
00:00:59,540 --> 00:01:02,640
Or, to be precise, two systems. System one and system two.

15
00:01:02,740 --> 00:01:03,880
Right. Two systems.

16
00:01:04,280 --> 00:01:16,140
And for most people, that distinction is absolute. System one is the fast one. It's instinct. It's the gut. It's the thing that makes you grab a cookie without thinking.

17
00:01:16,280 --> 00:01:17,160
The impulsive one.

18
00:01:17,160 --> 00:01:24,340
And system two is the slow one. It's logic. It's doing your taxes. It's painful deliberation.

19
00:01:24,680 --> 00:01:36,880
Exactly. It's the angel and the devil on your shoulder. It's the impulsive child and the responsible adult, all wrapped up in one neat package. And we all accept this. I mean, it feels true.

20
00:01:36,880 --> 00:01:48,140
It really does. When I'm driving on an empty highway, just zoned out, I'm in system one. But when I'm trying to calculate a 20% tip at a restaurant while my friends are all watching me.

21
00:01:48,180 --> 00:01:49,320
Oh, the pressure.

22
00:01:50,000 --> 00:01:53,520
That's pure system two. I can feel my brain creaking.

23
00:01:53,720 --> 00:01:59,020
It's a very, very seductive heuristic. It just matches our subjective experience so perfectly.

24
00:01:59,020 --> 00:02:11,580
Well, hold on to your heuristics, because today we're diving into a stack of documents that argues that entire framework is, well, maybe not wrong in its observations, but fundamentally wrong in its conclusions.

25
00:02:11,920 --> 00:02:12,820
This is a big one.

26
00:02:13,140 --> 00:02:22,300
We're looking at a paper titled The Myth of Dual Cognition by an author named Flexion, along with a really spicy explanatory note that came with it.

27
00:02:22,300 --> 00:02:28,940
And the thesis here is radical. It says there are not two systems. There is only one system.

28
00:02:29,140 --> 00:02:36,660
And that one system isn't like switching gears between two different engines. It's simply operating at different levels of resolution.

29
00:02:37,320 --> 00:02:41,480
Resolution. That is the key word we are going to be obsessing over today.

30
00:02:41,680 --> 00:02:47,460
This isn't just semantics, is it? This is not just a nerdy academic dispute about how we label things.

31
00:02:47,460 --> 00:02:51,740
Well, not at all. I mean, if the source material is correct, this changes everything.

32
00:02:51,740 --> 00:02:55,200
It changes how we view our own habits, how we view expertise.

33
00:02:55,940 --> 00:02:57,400
And maybe most importantly.

34
00:02:57,440 --> 00:03:01,840
And perhaps most importantly, it completely reframes the debate around artificial intelligence.

35
00:03:02,140 --> 00:03:05,780
That's the part that really got me. I mean, we're going to get to the AI stuff later.

36
00:03:05,920 --> 00:03:16,020
But the argument here basically says if we misunderstand how we think, we are definitely absolutely misunderstanding how machines think.

37
00:03:16,200 --> 00:03:17,180
That's a huge claim.

38
00:03:17,180 --> 00:03:23,800
We might be accusing AI of being dumb for the exact same reasons that actually make a human smart.

39
00:03:24,060 --> 00:03:30,160
It's a bold claim. But before we get to the robots, we have to deal with the humans.

40
00:03:30,760 --> 00:03:36,060
We have to unpack why we believe in the two system model in the first place. I mean, where did it come from?

41
00:03:36,060 --> 00:03:43,020
Right. Let's start with the status quo, because to understand the disruption, you have to understand what's being disrupted.

42
00:03:43,660 --> 00:03:47,520
When we talk about system one and system two, what are we actually describing?

43
00:03:48,060 --> 00:03:55,800
So the traditional definition, which really solidified with Kahneman and Tversky, posits this dichotomy.

44
00:03:56,620 --> 00:03:57,060
A split.

45
00:03:57,600 --> 00:03:59,420
On one side, you've got system one.

46
00:03:59,820 --> 00:04:00,420
The fast one.

47
00:04:00,420 --> 00:04:08,760
The fast one. It's described as automatic, intuitive, and crucially effortless. It's associative. It just happens.

48
00:04:09,060 --> 00:04:10,340
Give me a concrete example.

49
00:04:10,820 --> 00:04:23,120
Okay. So imagine you see a face. A friend walks into the room and they look absolutely furious. You don't have to break out a calculator. You don't have to measure the angle of their eyebrows or like the redness of their skin. You just know.

50
00:04:23,180 --> 00:04:23,860
It happens to you.

51
00:04:23,860 --> 00:04:31,540
Precisely. You don't do the work. The conclusion just arrives, fully formed. That's the hallmark of system one. It feels involuntary.

52
00:04:31,780 --> 00:04:33,420
Right. It feels like an input.

53
00:04:33,620 --> 00:04:33,780
Yeah.

54
00:04:33,840 --> 00:04:37,220
I didn't do the thinking. The thinking just appeared in my brain.

55
00:04:37,320 --> 00:04:48,800
Exactly. Now compare that to system two. This is slow, deliberative, reflective, and crucially effortful. It consumes resources. It's hard work.

56
00:04:48,800 --> 00:04:52,580
This is my calculating the tip moment. The cold sweat.

57
00:04:52,780 --> 00:04:58,440
Yes. Or if I ask you right now, what's 17 times 24?

58
00:04:58,720 --> 00:05:02,300
Oh, okay. Yeah. I'm stopping everything else. I'd have to like write it down.

59
00:05:02,400 --> 00:05:16,720
Exactly. You stop walking. Literally, studies show your pupils dilate, your blood pressure rises slightly. You have to grind through the steps. You have to hold the numbers in your head, perform an operation, check the result. That is system two.

60
00:05:16,720 --> 00:05:21,060
And it feels physically different. It feels like work. My brain hurts.

61
00:05:21,260 --> 00:05:27,420
It is work. And that's why the distinction feels so real. We have a fast and easy mode and a slow and hard mode. It's intuitive.

62
00:05:27,840 --> 00:05:38,760
But this is where the source material starts to poke holes in the theory. It points out that this didn't start as a claim about brain anatomy or, you know, different modules in the brain.

63
00:05:38,860 --> 00:05:39,720
No, not at all.

64
00:05:39,720 --> 00:05:45,900
It started as a phenomenological observation, which is a fancy way of saying it's just how it feels.

65
00:05:46,420 --> 00:05:58,320
Yes. And that history is vital here. This goes back way before Kahneman. We're talking about William James in the late 19th century. He was talking about the difference between habit and effort.

66
00:05:58,320 --> 00:05:58,980
Right.

67
00:05:59,080 --> 00:06:09,700
He wasn't doing fMRI scans. He had no idea which neurons were firing where. He was just a brilliant observer describing the human experience from the inside out.

68
00:06:09,920 --> 00:06:16,220
So originally, system one and system two were just adjectives. They were just ways of describing a process.

69
00:06:16,800 --> 00:06:25,440
Exactly. There were descriptors. You could say, I'm thinking quickly right now or I'm thinking slowly. I'm thinking effortlessly or I'm thinking effortfully.

70
00:06:25,440 --> 00:06:27,900
It's a description of the texture of thought.

71
00:06:28,440 --> 00:06:34,360
But then something happened. The paper calls it, and I love this term, ontological drift.

72
00:06:34,880 --> 00:06:37,260
Ontological drift. It's such a great phrase.

73
00:06:37,720 --> 00:06:39,880
It sounds like something from a sci-fi movie.

74
00:06:40,800 --> 00:06:46,340
Captain, we're experiencing ontological drift. But what does it actually mean in this context?

75
00:06:46,660 --> 00:06:53,780
Well, it's a philosophical term. Ontology is the study of what exists, what things fundamentally are.

76
00:06:53,780 --> 00:06:56,600
And drift is the error.

77
00:06:57,620 --> 00:07:07,000
So ontological drift happens when we take a descriptive, label-like, fast thinking, and we drift into believing it is a physical thing.

78
00:07:07,200 --> 00:07:09,020
We turn the adjective into a noun.

79
00:07:09,260 --> 00:07:12,840
We turn the adjective into a noun, and then we turn that noun into a machine.

80
00:07:13,100 --> 00:07:19,960
We move from saying, this thought process is fast, to believing there is a fast-thinking machine inside my head.

81
00:07:19,960 --> 00:07:23,960
Okay, I want to use an analogy here to make sure I've got this locked down.

82
00:07:24,860 --> 00:07:28,060
The paper mentions cars, but let's look at a runner.

83
00:07:28,200 --> 00:07:29,600
A runner. Okay, I'm with you.

84
00:07:29,760 --> 00:07:31,080
Say you have an Olympic athlete.

85
00:07:31,300 --> 00:07:31,540
Uh-huh.

86
00:07:31,640 --> 00:07:36,800
Sometimes they sprint. They're going 100% max effort, exploding off the blocks.

87
00:07:36,980 --> 00:07:38,540
That's fast. That's intense.

88
00:07:38,800 --> 00:07:41,820
System two in the old model. All-out effort.

89
00:07:42,040 --> 00:07:45,620
Right. Other times, they're just jogging a warm-up lap.

90
00:07:45,960 --> 00:07:49,140
That's slow. That's easy. They can chat while they do it. System one.

91
00:07:49,200 --> 00:07:50,420
Right. Two different speeds.

92
00:07:50,940 --> 00:07:55,940
Now, ontological drift would be like watching that athlete and saying, oh, I get it.

93
00:07:56,140 --> 00:08:00,200
He has two separate bodies. He has a sprinting body and a jogging body.

94
00:08:00,400 --> 00:08:05,460
And when he wants to go fast, he climbs out of the jogging body and gets into the sprinting body.

95
00:08:05,460 --> 00:08:08,720
That sounds completely absurd when you put it that way.

96
00:08:08,720 --> 00:08:17,100
It is absurd. It's the same legs. It's the same lungs. It's the same heart. It's the same person just operating at a different level of intensity.

97
00:08:18,480 --> 00:08:23,200
But the source material argues that this is exactly what we have done with our brains.

98
00:08:23,320 --> 00:08:24,860
That is a perfect analogy.

99
00:08:24,860 --> 00:08:31,820
We have created a homunculus, a little person inside our heads, or in this case, two little people.

100
00:08:32,400 --> 00:08:43,720
We act as if there is a system one guy who is impulsive and kind of dumb and a system two guy who is smart and rational and they are fighting for control of the steering wheel.

101
00:08:44,120 --> 00:08:46,400
And the source says this isn't just a harmless metaphor.

102
00:08:46,760 --> 00:08:50,600
It leads to what the author calls the moralization of cognition.

103
00:08:50,600 --> 00:09:00,260
And this is a really, really important point. Because once you split the brain into two characters, you inevitably start assigning them moral roles. Good versus evil.

104
00:09:00,520 --> 00:09:02,340
We cast system one as the villain.

105
00:09:02,620 --> 00:09:08,440
We do. In the current worldview, the standard Kahneman worldview system one is the problem.

106
00:09:08,840 --> 00:09:14,960
It's the source of all our biases. It's the thing that makes you racist or sexist or lazy.

107
00:09:15,660 --> 00:09:18,940
It's the thing that eats the donut when you're on a diet.

108
00:09:18,940 --> 00:09:22,760
It's the lizard brain. The primitive part we have to overcome.

109
00:09:23,280 --> 00:09:27,140
And system two is the hero. System two is the adult in the room.

110
00:09:27,540 --> 00:09:30,100
It's rational. It's normative. It's corrective.

111
00:09:30,480 --> 00:09:35,640
We tell ourselves things like, I need to engage my system two to stop doom scrolling.

112
00:09:36,080 --> 00:09:39,940
Or, I need to use logic to overcome my unconscious bias.

113
00:09:40,780 --> 00:09:45,000
It becomes a morality play. The beast versus the angel fighting inside your skull.

114
00:09:45,000 --> 00:09:48,260
Exactly. But the source argues this is a trap.

115
00:09:48,820 --> 00:09:55,860
It suggests that by splitting them into good cop and bad cop, we completely miss the mechanical reality of how thinking actually works.

116
00:09:56,140 --> 00:09:59,400
We are ignoring how the runner actually learns to run in the first place.

117
00:09:59,480 --> 00:10:01,100
So let's get to that mechanical reality.

118
00:10:01,240 --> 00:10:04,800
If there aren't two guys in my head fighting for the steering wheel, what is actually happening?

119
00:10:05,140 --> 00:10:08,160
This brings us to the core thesis of this whole deep dive.

120
00:10:08,780 --> 00:10:10,320
Aspect relegation theory.

121
00:10:10,320 --> 00:10:19,000
Aspect relegation theory. It sounds a bit technical, but I promise you, the concept is beautifully simple once you grasp it.

122
00:10:19,080 --> 00:10:21,160
Okay. Bring it down for us. What's the elevator pitch?

123
00:10:21,520 --> 00:10:24,980
The theory proposes that system one is not a separate thing at all.

124
00:10:25,220 --> 00:10:29,200
It is simply system two processes that have become automated.

125
00:10:29,840 --> 00:10:33,080
Good. Say that again. That's the whole ballgame right there.

126
00:10:33,180 --> 00:10:35,360
System one is just automated system two.

127
00:10:35,360 --> 00:10:38,340
Yes. There is a key quote in the paper I want to read.

128
00:10:39,020 --> 00:10:48,020
System one is the residual form of system two processes that have become automated through repetition, stabilization, and attentional compression.

129
00:10:48,300 --> 00:10:52,760
The residual form. I love that phrasing. It's like what's left over after a process.

130
00:10:52,940 --> 00:10:54,980
I was thinking about it like cooking.

131
00:10:55,640 --> 00:10:58,840
Specifically, think about making a reduction sauce.

132
00:10:59,000 --> 00:11:01,140
Okay. I'm listening. You've got my attention.

133
00:11:01,140 --> 00:11:07,060
You start with a huge pot of liquid stock, wine, herbs, onions, all this stuff.

134
00:11:07,260 --> 00:11:08,780
That is your system two.

135
00:11:09,280 --> 00:11:12,420
It is voluminous. It has all these distinct ingredients.

136
00:11:12,600 --> 00:11:14,520
You can see the individual onions floating in it.

137
00:11:14,580 --> 00:11:16,320
It takes up a lot of space in the pot.

138
00:11:16,640 --> 00:11:17,780
It takes a lot of attention.

139
00:11:18,080 --> 00:11:20,760
That's the high effort phase. The deliberative phase.

140
00:11:21,080 --> 00:11:24,000
Right. Now you simmer it for hours.

141
00:11:24,420 --> 00:11:25,220
You reduce it.

142
00:11:25,400 --> 00:11:28,920
You apply heat and time, which is like practice and repetition.

143
00:11:28,920 --> 00:11:38,140
The water evaporates, the volume shrinks, and eventually you are left with this thick, intense, glossy glaze at the bottom of the pan.

144
00:11:38,900 --> 00:11:40,460
That is system one.

145
00:11:40,560 --> 00:11:41,640
It's the same stuff.

146
00:11:41,720 --> 00:11:43,460
It is chemically the same stuff.

147
00:11:43,560 --> 00:11:48,240
It's the same flavor profile, just unbelievably compressed.

148
00:11:48,520 --> 00:11:50,460
You can't see the individual onions anymore.

149
00:11:50,560 --> 00:11:53,160
They've dissolved into the very essence of the sauce.

150
00:11:53,380 --> 00:11:55,200
But their flavor is still there.

151
00:11:55,660 --> 00:11:57,600
System one is in a different liquid.

152
00:11:57,600 --> 00:12:00,880
It's just the concentrated residue of the original reasoning.

153
00:12:01,180 --> 00:12:01,360
Wow.

154
00:12:01,600 --> 00:12:01,840
Okay.

155
00:12:01,940 --> 00:12:06,820
So my gut feeling isn't some magic bolt of lightning from a different brain.

156
00:12:07,080 --> 00:12:07,320
Yeah.

157
00:12:07,560 --> 00:12:11,020
It's a reduction sauce of all my past experiences and calculations.

158
00:12:11,200 --> 00:12:11,660
Exactly.

159
00:12:11,860 --> 00:12:18,600
And the mechanism for how we get from the big pot of soup to that tiny bit of sauce is what the author calls aspect relegation.

160
00:12:19,340 --> 00:12:20,420
Let's unpack those words.

161
00:12:20,820 --> 00:12:21,600
Aspects and relegation.

162
00:12:22,280 --> 00:12:22,960
What's an aspect?

163
00:12:22,960 --> 00:12:30,180
So any cognitive process, reasoning, driving, doing math, whatever, is made up of aspects.

164
00:12:30,740 --> 00:12:34,240
These are the intermediate steps, the little subroutines, the conditional branches.

165
00:12:34,740 --> 00:12:36,960
If X happens, then I do Y.

166
00:12:37,500 --> 00:12:38,360
The error checks.

167
00:12:38,600 --> 00:12:39,860
Did I carry the one correctly?

168
00:12:40,340 --> 00:12:42,240
Is that light red or is it green?

169
00:12:42,580 --> 00:12:47,200
The little distinct pieces of the thought, the individual instructions in the recipe.

170
00:12:47,200 --> 00:12:47,720
Perfect.

171
00:12:48,020 --> 00:12:48,220
Right.

172
00:12:49,020 --> 00:12:56,120
When a task is new, when you're first making that sauce, you are in what the author calls high resolution.

173
00:12:56,980 --> 00:12:59,300
You have to see every single aspect.

174
00:12:59,500 --> 00:13:02,780
You are monitoring the onions, the heat, the spoon, the timing.

175
00:13:03,200 --> 00:13:05,900
You are consciously aware of every variable.

176
00:13:06,120 --> 00:13:07,740
Because you don't know which ones matter yet.

177
00:13:07,820 --> 00:13:08,460
You're a novice.

178
00:13:08,560 --> 00:13:09,660
You haven't figured out the pattern.

179
00:13:09,660 --> 00:13:10,180
Exactly.

180
00:13:10,780 --> 00:13:17,080
But as you repeat the task, as it becomes stable and predictable, you start to relegate those aspects.

181
00:13:17,240 --> 00:13:18,380
You push them into the background.

182
00:13:18,560 --> 00:13:20,780
You stop paying attention to the intermediate steps.

183
00:13:21,020 --> 00:13:23,600
You just see the input, ingredients, and the output.

184
00:13:24,020 --> 00:13:24,920
Delicious sauce.

185
00:13:25,260 --> 00:13:27,320
The middle part becomes automatic.

186
00:13:27,640 --> 00:13:33,300
This brings us to the central metaphor of the paper, which I think is just brilliant for the digital age.

187
00:13:33,640 --> 00:13:34,960
It's not about engines.

188
00:13:35,080 --> 00:13:36,040
It's about resolution.

189
00:13:36,500 --> 00:13:36,860
Yes.

190
00:13:36,860 --> 00:13:42,520
This moves us away from the two engines idea and toward a one screen idea.

191
00:13:42,840 --> 00:13:44,660
It's a much better way to think about it.

192
00:13:44,800 --> 00:13:47,880
The argument is that the brain isn't switching engines.

193
00:13:48,500 --> 00:13:50,800
It's changing the resolution of its attention.

194
00:13:51,260 --> 00:13:56,480
Think about streaming a video on YouTube or Netflix or whatever you use.

195
00:13:56,640 --> 00:13:58,120
You are watching a movie.

196
00:13:58,480 --> 00:14:02,000
It's the same file, the same plot, same character, same runtime.

197
00:14:02,760 --> 00:14:05,820
That file represents the core reasoning process.

198
00:14:05,820 --> 00:14:09,480
Now, imagine your internet connection is a bit spotty.

199
00:14:09,940 --> 00:14:11,320
You have limited bandwidth.

200
00:14:11,880 --> 00:14:12,720
Limited attention.

201
00:14:13,280 --> 00:14:14,800
Limited cognitive resources.

202
00:14:14,960 --> 00:14:15,380
Exactly.

203
00:14:15,600 --> 00:14:16,720
Bandwidth is attention.

204
00:14:16,880 --> 00:14:20,840
If you have limited bandwidth, the stream automatically drops down to 240p.

205
00:14:21,040 --> 00:14:21,700
It's blocky.

206
00:14:21,860 --> 00:14:22,300
It's blurry.

207
00:14:22,300 --> 00:14:26,140
You can't see the texture of the actor's shirt or the leaves on the trees.

208
00:14:26,720 --> 00:14:29,920
But, and this is key, it loads instantly.

209
00:14:30,160 --> 00:14:32,240
It plays smoothly without buffering.

210
00:14:32,400 --> 00:14:33,360
That is system one.

211
00:14:33,880 --> 00:14:36,100
Low resolution, but high speed and low effort.

212
00:14:36,480 --> 00:14:36,920
Precisely.

213
00:14:37,120 --> 00:14:39,340
But let's say you need to see a specific detail.

214
00:14:39,540 --> 00:14:44,000
There's a clue written on a tiny piece of paper in a movie, and you need to read it to solve the mystery.

215
00:14:44,000 --> 00:14:46,800
You can't read it in 240p, so what do you do?

216
00:14:47,120 --> 00:14:50,780
I pause it, go to the settings, and I crank it up to 4K.

217
00:14:51,440 --> 00:14:54,880
You crank up the resolution, and what happens immediately?

218
00:14:54,980 --> 00:14:55,380
Buffers.

219
00:14:55,600 --> 00:14:57,120
The little spinning wheel of death comes up.

220
00:14:57,460 --> 00:14:58,240
It takes time.

221
00:14:58,720 --> 00:14:59,720
It chews up my data.

222
00:14:59,880 --> 00:15:01,660
It consumes massive bandwidth.

223
00:15:02,420 --> 00:15:03,560
That is system two.

224
00:15:03,900 --> 00:15:05,120
It's not a different movie.

225
00:15:05,360 --> 00:15:11,780
It's the same data, just rendered at a much higher fidelity, because you need to inspect the details, the aspects.

226
00:15:11,780 --> 00:15:14,760
This completely demystifies the experience for me.

227
00:15:15,280 --> 00:15:18,300
When I'm thinking hard, I'm not using a different brain.

228
00:15:18,460 --> 00:15:22,120
I'm just forcing my mental video stream into 4K.

229
00:15:22,560 --> 00:15:24,180
I'm looking at the individual pixels.

230
00:15:24,500 --> 00:15:28,040
And when you are thinking fast, you are accepting the blur.

231
00:15:28,600 --> 00:15:33,440
You are trusting that the general shape of the image is enough to get by for now.

232
00:15:33,880 --> 00:15:34,920
Trusting the blur.

233
00:15:35,400 --> 00:15:36,460
That's a great way to put it.

234
00:15:36,460 --> 00:15:42,520
I want to ground this in a real-world example, because the paper gives a couple of really good ones that make this concrete.

235
00:15:43,080 --> 00:15:44,900
The first one is the commuter.

236
00:15:45,160 --> 00:15:45,620
The commuter.

237
00:15:45,800 --> 00:15:46,080
Yes.

238
00:15:46,160 --> 00:15:51,480
This is a classic example of habit formation, or what this theory would call relegation.

239
00:15:51,800 --> 00:15:52,560
Walk us through it.

240
00:15:52,560 --> 00:15:56,600
Okay, so imagine someone who lives in a big city, like New York or London.

241
00:15:57,380 --> 00:16:01,420
They walk to the subway station every single morning for five years, same route.

242
00:16:01,800 --> 00:16:05,880
They walk out their front door, turn left, cross two streets.

243
00:16:06,120 --> 00:16:10,360
They dodge that same puddle that always forms on the corner after it rains.

244
00:16:10,500 --> 00:16:11,500
We all have that puddle.

245
00:16:11,500 --> 00:16:18,860
They swipe their transit card, walk down the stairs, and stand in the exact same spot on the platform every day.

246
00:16:19,300 --> 00:16:24,720
Now, if you stop them on the platform and ask, how did you get here, describe your walk.

247
00:16:24,780 --> 00:16:25,420
What do they say?

248
00:16:25,540 --> 00:16:27,400
They'll say, I don't know.

249
00:16:27,460 --> 00:16:28,860
I was thinking about dinner tonight.

250
00:16:28,920 --> 00:16:29,960
I was listening to a podcast.

251
00:16:30,280 --> 00:16:31,820
I just, I kind of arrived.

252
00:16:32,040 --> 00:16:33,720
It feels like teleportation.

253
00:16:33,800 --> 00:16:35,260
It feels completely automatic.

254
00:16:35,440 --> 00:16:36,860
It feels like pure system one.

255
00:16:37,000 --> 00:16:37,500
It does.

256
00:16:38,000 --> 00:16:40,720
But let's look at the reality of what they just did.

257
00:16:41,520 --> 00:16:46,640
Navigating a busy city street is an incredibly complex physics and logic problem.

258
00:16:47,240 --> 00:16:51,480
They had to calculate the velocity of oncoming cars to cross the street safely.

259
00:16:51,800 --> 00:16:56,040
They had to navigate complex spatial geometry to turn the corners.

260
00:16:56,600 --> 00:17:00,780
They had to use fine motor skills to swick their car just right.

261
00:17:00,780 --> 00:17:08,760
I mean, if you tried to get a Boston Dynamics robot to do that, it would take millions of lines of code, and it would still probably fall into the puddle.

262
00:17:08,880 --> 00:17:09,300
Exactly.

263
00:17:09,300 --> 00:17:14,900
So, did the commuter use a primitive lizard brain to do all that complex math?

264
00:17:15,120 --> 00:17:15,480
No.

265
00:17:16,060 --> 00:17:22,980
According to this theory, they used the exact same reasoning they used the very first day they moved into that apartment.

266
00:17:23,360 --> 00:17:26,460
But the first day they moved in, it was hard.

267
00:17:26,860 --> 00:17:27,640
It was stressful.

268
00:17:27,640 --> 00:17:29,980
The first day was high resolution.

269
00:17:30,200 --> 00:17:31,020
It was 4K.

270
00:17:31,360 --> 00:17:33,160
Okay, what's the street sign say?

271
00:17:33,280 --> 00:17:34,300
Is this a one-way street?

272
00:17:34,500 --> 00:17:36,000
Okay, there's a puddle on the left side.

273
00:17:36,040 --> 00:17:36,980
I need to remember that.

274
00:17:37,280 --> 00:17:38,680
Where did I put my transit card?

275
00:17:39,760 --> 00:17:42,760
They were streaming every single detail.

276
00:17:43,020 --> 00:17:47,840
But after five years, they have relegated all those aspects.

277
00:17:48,020 --> 00:17:49,020
They've compressed them.

278
00:17:49,020 --> 00:17:50,200
They have solved the problem.

279
00:17:50,480 --> 00:17:53,360
The solution is cached, to use a computer term.

280
00:17:53,620 --> 00:17:58,640
They aren't re-solving the complex problem of how to get to the station every morning.

281
00:17:58,880 --> 00:18:01,540
They are just executing the stored program.

282
00:18:01,940 --> 00:18:05,240
The reasoning, the physics, the geometry is all still there.

283
00:18:05,520 --> 00:18:09,460
It's just been compressed into a smooth 240p stream.

284
00:18:09,460 --> 00:18:14,100
But, and here is the kicker, and this is where the theory really shines for me.

285
00:18:14,620 --> 00:18:19,320
What happens if one day they walk up to the station and the entrance is closed for construction?

286
00:18:19,800 --> 00:18:22,740
A big sign, yellow tape, the words.

287
00:18:22,760 --> 00:18:23,800
Oh yeah, boom.

288
00:18:24,260 --> 00:18:26,480
The automaticity shatters instantly.

289
00:18:27,300 --> 00:18:28,440
Relegation stops.

290
00:18:28,540 --> 00:18:29,920
The blur snaps into focus.

291
00:18:30,120 --> 00:18:32,460
The commuter stops dead in their tracks.

292
00:18:32,660 --> 00:18:33,520
They look around.

293
00:18:33,640 --> 00:18:35,840
The autopilot disengages completely.

294
00:18:36,560 --> 00:18:38,040
Suddenly they are reading signs.

295
00:18:38,140 --> 00:18:39,140
They are checking their watch.

296
00:18:39,140 --> 00:18:41,160
They are pulling out their phone to check the map.

297
00:18:41,240 --> 00:18:42,480
They're looking for a bus stop.

298
00:18:42,640 --> 00:18:44,240
They're calculating a new route.

299
00:18:44,380 --> 00:18:45,760
They are back in 4K.

300
00:18:45,880 --> 00:18:46,120
Yeah.

301
00:18:46,420 --> 00:18:47,600
Full buffering mode.

302
00:18:47,720 --> 00:18:49,580
They are back in what we call system 2.

303
00:18:50,220 --> 00:18:53,540
But notice, they didn't switch brains.

304
00:18:53,740 --> 00:18:57,900
They just re-promoted the aspects of navigation back into conscious attention.

305
00:18:58,420 --> 00:19:00,060
Their brain got an error message.

306
00:19:00,440 --> 00:19:02,000
The cast solution has failed.

307
00:19:02,440 --> 00:19:05,080
I need to open the source file and look at the code again.

308
00:19:05,480 --> 00:19:08,060
That transition is so seamless we don't even notice it.

309
00:19:08,060 --> 00:19:10,680
It's like the auto quality setting on YouTube.

310
00:19:11,060 --> 00:19:12,480
My phone just does it.

311
00:19:12,680 --> 00:19:15,220
It fluctuates based on need, on bandwidth.

312
00:19:15,720 --> 00:19:19,160
And that fluctuation is the essence of intelligence.

313
00:19:20,160 --> 00:19:22,980
It's not about being in system 2 all the time.

314
00:19:23,240 --> 00:19:25,920
That would be completely paralyzing.

315
00:19:26,060 --> 00:19:28,100
And it's not about being in system 1 all the time.

316
00:19:28,180 --> 00:19:29,500
That would be reckless.

317
00:19:29,800 --> 00:19:30,040
Right.

318
00:19:30,120 --> 00:19:32,800
It's about the dynamic regulation of that resolution.

319
00:19:33,440 --> 00:19:36,340
Knowing when to zoom in and when to zoom out.

320
00:19:36,340 --> 00:19:38,300
I want to talk about the driving example, too.

321
00:19:38,380 --> 00:19:40,440
Because I think that connects to the feeling of effort.

322
00:19:40,640 --> 00:19:43,260
We mentioned earlier that system 2 feels like work.

323
00:19:43,500 --> 00:19:43,740
Yes.

324
00:19:44,560 --> 00:19:49,160
Effort is the defining characteristic of the slow system in the old model.

325
00:19:49,420 --> 00:19:53,320
The paper makes a really interesting point about what effort actually is.

326
00:19:54,020 --> 00:19:57,400
Because we tend to equate effort with being smart.

327
00:19:58,120 --> 00:20:01,320
You know, I'm thinking really hard, so I must be being rational.

328
00:20:01,320 --> 00:20:01,720
Right.

329
00:20:01,920 --> 00:20:03,860
We wear effort like a badge of honor.

330
00:20:04,440 --> 00:20:10,440
But the author here argues that effort is simply a bandwidth warning light on your mental dashboard.

331
00:20:10,780 --> 00:20:12,160
A bandwidth warning light.

332
00:20:12,440 --> 00:20:13,120
I like that.

333
00:20:13,340 --> 00:20:14,660
Think about the novice driver.

334
00:20:15,020 --> 00:20:17,460
The 16-year-old with their learner's permit.

335
00:20:17,800 --> 00:20:19,360
I remember when I was learning to drive.

336
00:20:19,500 --> 00:20:21,200
It was physically exhausting.

337
00:20:21,860 --> 00:20:22,360
Oh, totally.

338
00:20:22,760 --> 00:20:27,460
I remember gripping the steering wheel so hard my knuckles were white I couldn't have the radio on.

339
00:20:27,460 --> 00:20:30,120
My mom tried to talk to me from the passenger seat.

340
00:20:30,200 --> 00:20:30,900
I'd snap at her.

341
00:20:31,180 --> 00:20:31,540
Quiet.

342
00:20:31,800 --> 00:20:32,360
I'm urging.

343
00:20:33,000 --> 00:20:33,440
Exactly.

344
00:20:34,040 --> 00:20:36,020
You were in maximum high resolution.

345
00:20:36,340 --> 00:20:39,060
You were tracking 50 different variables explicitly.

346
00:20:39,340 --> 00:20:41,760
The pressure of your foot on the gas pedal.

347
00:20:42,180 --> 00:20:44,560
The exact angle of the rearview mirror.

348
00:20:44,900 --> 00:20:46,460
The distance to the curb.

349
00:20:46,620 --> 00:20:48,620
The speed of the car behind you.

350
00:20:49,000 --> 00:20:53,240
You were holding all these aspects in your active working memory.

351
00:20:53,440 --> 00:20:54,240
Your RAM.

352
00:20:54,580 --> 00:20:55,920
And that burns energy.

353
00:20:55,920 --> 00:20:57,520
That is the effort.

354
00:20:57,980 --> 00:21:02,060
That's the feeling of my brain's CPU running at 100%.

355
00:21:02,060 --> 00:21:03,060
That is the effort.

356
00:21:03,240 --> 00:21:05,960
You are keeping all those plates spinning manually.

357
00:21:06,320 --> 00:21:07,300
Now, look at you today.

358
00:21:07,500 --> 00:21:09,080
You probably drive to work.

359
00:21:09,160 --> 00:21:10,000
You're drinking coffee.

360
00:21:10,240 --> 00:21:11,480
You're listening to this podcast.

361
00:21:11,780 --> 00:21:13,280
You're arguing with the radio host.

362
00:21:13,460 --> 00:21:15,380
You are barely looking at the road.

363
00:21:15,640 --> 00:21:17,320
Hopefully looking at the road a little bit.

364
00:21:17,460 --> 00:21:18,340
For legal reasons.

365
00:21:18,720 --> 00:21:19,740
Well, yes.

366
00:21:20,360 --> 00:21:23,260
But you aren't consciously thinking about the pedal pressure.

367
00:21:23,260 --> 00:21:26,340
You aren't calculating the angle of the turn.

368
00:21:26,780 --> 00:21:28,520
You have relegated those aspects.

369
00:21:28,980 --> 00:21:29,920
They have been compressed.

370
00:21:30,460 --> 00:21:34,520
The reasoning structure, how to drive a car, hasn't vanished.

371
00:21:34,760 --> 00:21:36,340
You still know how to drive.

372
00:21:36,460 --> 00:21:38,600
In fact, you know it better than the 16-year-old.

373
00:21:38,900 --> 00:21:41,960
But because I know it better, it feels like I'm doing less.

374
00:21:42,040 --> 00:21:42,400
It feels effortless.

375
00:21:42,400 --> 00:21:42,960
Effortless.

376
00:21:43,120 --> 00:21:43,600
Exactly.

377
00:21:44,220 --> 00:21:45,540
And this is the paradox.

378
00:21:46,200 --> 00:21:49,940
As you become an expert, the feeling of effort decreases.

379
00:21:50,480 --> 00:21:54,080
You move from what feels like System 2 to what feels like System 1.

380
00:21:54,460 --> 00:22:02,280
So if we follow the old model, we'd have to say the expert driver is thinking less or being less rational than the novice.

381
00:22:02,580 --> 00:22:03,840
Which is just ridiculous.

382
00:22:04,100 --> 00:22:05,900
The expert is obviously a better driver.

383
00:22:05,980 --> 00:22:06,300
Right.

384
00:22:06,600 --> 00:22:08,600
The expert has simply compiled the code.

385
00:22:08,600 --> 00:22:13,180
The novice is running the code line by line, interpreting it as they go in real time.

386
00:22:13,320 --> 00:22:14,480
That's slow and hard.

387
00:22:14,900 --> 00:22:17,640
The expert is running the compiled, executable file.

388
00:22:18,100 --> 00:22:20,700
It runs instantly and silently in the background.

389
00:22:21,280 --> 00:22:26,240
This leads us naturally into the most mystical, most romanticized part of human cognition.

390
00:22:27,180 --> 00:22:27,700
Intuition.

391
00:22:27,920 --> 00:22:28,840
Ah, yes.

392
00:22:29,380 --> 00:22:30,760
The magic of the gut feeling.

393
00:22:31,120 --> 00:22:32,580
We love to talk about intuition.

394
00:22:33,440 --> 00:22:35,960
Especially in business or creative fields.

395
00:22:35,960 --> 00:22:38,920
We idolize the CEO who says,

396
00:22:39,140 --> 00:22:40,880
I didn't look at the data.

397
00:22:41,380 --> 00:22:42,640
I just went with my gut.

398
00:22:43,480 --> 00:22:45,060
We treat it like a superpower.

399
00:22:45,360 --> 00:22:48,440
Like it's a direct line to some cosmic truth.

400
00:22:48,540 --> 00:22:51,380
We treat it as if it is fundamentally distinct from reasoning.

401
00:22:51,840 --> 00:22:52,980
You know, don't overthink it.

402
00:22:53,040 --> 00:22:55,100
Just feel it as if they're opposites.

403
00:22:55,380 --> 00:22:59,140
But Flixin, the author here, is basically the party pooper.

404
00:22:59,300 --> 00:23:01,760
He's the magician revealing how the trick is done.

405
00:23:02,180 --> 00:23:04,500
What is the definition of intuition in this theory?

406
00:23:04,500 --> 00:23:06,840
It is remarkably unmagical.

407
00:23:07,060 --> 00:23:08,920
And I think that's why it's so powerful.

408
00:23:09,180 --> 00:23:15,900
The paper defines intuition as successful concealment of previously explicit inferential structure.

409
00:23:16,160 --> 00:23:17,020
Successful concealment.

410
00:23:17,100 --> 00:23:18,840
That is so dry.

411
00:23:19,040 --> 00:23:19,600
So clinical.

412
00:23:19,780 --> 00:23:21,280
It is dry, but it's so accurate.

413
00:23:21,380 --> 00:23:27,200
It basically says intuition is just reasoning that has been compressed so tightly that you can no longer see the steps.

414
00:23:27,280 --> 00:23:28,580
You've hidden the work from yourself.

415
00:23:28,760 --> 00:23:32,560
There's a phrase in the source that I highlighted three times because it's so good.

416
00:23:32,560 --> 00:23:35,740
Intuition is just yesterday's reasoning.

417
00:23:36,420 --> 00:23:37,340
Efficiently forgotten.

418
00:23:38,020 --> 00:23:39,040
Yesterday's reasoning.

419
00:23:39,520 --> 00:23:40,400
Efficiently forgotten.

420
00:23:41,220 --> 00:23:42,000
I love that.

421
00:23:42,080 --> 00:23:42,260
Yeah.

422
00:23:42,500 --> 00:23:45,320
It's what you'd call a deflationary view of intuition.

423
00:23:45,740 --> 00:23:47,800
Takes all the hot air out of the balloon.

424
00:23:48,060 --> 00:23:49,840
So let's apply this.

425
00:23:49,980 --> 00:23:50,140
Yeah.

426
00:23:50,220 --> 00:23:51,640
Say I'm a seasoned detective.

427
00:23:51,900 --> 00:23:53,460
I've been on the force for 30 years.

428
00:23:53,460 --> 00:23:55,520
I walk into a crime scene.

429
00:23:55,640 --> 00:23:57,620
Within five seconds, I look around.

430
00:23:57,720 --> 00:23:59,380
I say, the husband did it.

431
00:23:59,560 --> 00:23:59,680
Right.

432
00:24:00,160 --> 00:24:02,900
In the movies, that's presented as a psychic flash.

433
00:24:03,160 --> 00:24:06,180
A moment of pure, unexplainable genius.

434
00:24:06,600 --> 00:24:10,240
But under aspect relegation theory, what just happened in my brain?

435
00:24:10,360 --> 00:24:14,140
What happened is that your brain scanned the room in high speed, low resolution.

436
00:24:14,800 --> 00:24:20,020
You noticed, without consciously realizing it, that there was no sign of forced entry.

437
00:24:20,020 --> 00:24:24,260
You noticed the husband's body language was defensive, not grieving.

438
00:24:24,560 --> 00:24:28,600
You noticed a glass on the table was wiped clean when everything else was dusty.

439
00:24:28,840 --> 00:24:32,380
I saw the onions in the reduction sauce, but I just tasted the final sauce.

440
00:24:32,480 --> 00:24:33,000
Exactly.

441
00:24:33,200 --> 00:24:35,360
You processed a dozen clues in parallel.

442
00:24:36,100 --> 00:24:40,700
Clues that 20 years ago, as a rookie detective, you would have had to write down in a notebook

443
00:24:40,700 --> 00:24:42,460
and stare at for hours.

444
00:24:42,740 --> 00:24:42,900
Okay.

445
00:24:43,100 --> 00:24:43,700
Clue one.

446
00:24:43,960 --> 00:24:44,980
No forced entry.

447
00:24:45,260 --> 00:24:45,840
Clue two.

448
00:24:47,180 --> 00:24:48,640
Weird body language.

449
00:24:48,640 --> 00:24:50,240
You would have had to do it in 4K.

450
00:24:50,820 --> 00:24:51,160
Yes.

451
00:24:51,620 --> 00:24:57,480
But because you've done this a thousand times, your brain ran the crime scene analysis script

452
00:24:57,480 --> 00:25:01,460
in the background and just handed you the output, the final answer.

453
00:25:02,120 --> 00:25:02,620
Husband.

454
00:25:03,800 --> 00:25:10,400
So the gut feeling is actually just a hyper-fast summary of evidence presented to your consciousness

455
00:25:10,400 --> 00:25:11,200
as a feeling.

456
00:25:11,500 --> 00:25:11,940
Correct.

457
00:25:11,940 --> 00:25:17,900
It feels like the answer just appears only because the intermediate work is hidden from

458
00:25:17,900 --> 00:25:18,820
your conscious view.

459
00:25:19,060 --> 00:25:20,160
You don't see the factory.

460
00:25:20,580 --> 00:25:23,180
You just get the finished product delivered to your doorstep.

461
00:25:23,660 --> 00:25:29,060
This really explains why trusting your gut is only good advice if you're actually an expert

462
00:25:29,060 --> 00:25:29,880
in that domain.

463
00:25:30,120 --> 00:25:30,700
Oh, absolutely.

464
00:25:30,920 --> 00:25:32,340
That is the critical takeaway.

465
00:25:33,020 --> 00:25:37,280
If a novice in a field tries to trust their gut, they are just guessing.

466
00:25:37,680 --> 00:25:39,440
They are just expressing their biases.

467
00:25:39,440 --> 00:25:41,520
They don't have the compressed reasoning.

468
00:25:41,840 --> 00:25:43,600
They don't have the reduction sauce.

469
00:25:43,760 --> 00:25:45,300
All they have is hot water.

470
00:25:45,440 --> 00:25:46,520
There's nothing to reduce.

471
00:25:46,740 --> 00:25:47,280
Perfectly put.

472
00:25:47,380 --> 00:25:49,080
You have to do the work in system two.

473
00:25:49,320 --> 00:25:54,120
You have to do the slow, deliberative learning before you can earn the speed of system one.

474
00:25:54,280 --> 00:25:57,620
You can't have the glaze without the long, slow reduction.

475
00:25:58,020 --> 00:25:58,260
Okay.

476
00:25:58,360 --> 00:26:00,020
So we've dismantled the human mind.

477
00:26:00,400 --> 00:26:05,980
We've turned intuition into zip files and expert driving into compiled code.

478
00:26:05,980 --> 00:26:08,560
But the source material doesn't stop there.

479
00:26:09,160 --> 00:26:13,600
It takes this theory and throws a grenade right into the middle of the biggest debate

480
00:26:13,600 --> 00:26:14,880
in technology right now.

481
00:26:15,020 --> 00:26:16,180
Artificial intelligence.

482
00:26:16,440 --> 00:26:18,440
Specifically large language models.

483
00:26:18,920 --> 00:26:21,420
Chachi-BT, Claude, Gemini.

484
00:26:22,160 --> 00:26:24,820
The AIs that we are all talking about constantly.

485
00:26:25,040 --> 00:26:29,940
This is where the paper gets a little combative, especially in that explanatory note.

486
00:26:30,340 --> 00:26:34,120
It explicitly calls out the Gary Marcus style of critique.

487
00:26:34,120 --> 00:26:37,480
We need to set the stage here for anyone who might not be familiar.

488
00:26:37,780 --> 00:26:43,260
Who is Gary Marcus and what is the standard critique of these AI models?

489
00:26:43,400 --> 00:26:47,460
Because I hear this argument all the time, even from people who don't know who Gary Marcus is.

490
00:26:47,540 --> 00:26:54,420
So Gary Marcus is a cognitive scientist and a very vocal, very prominent critic of the current AI hype.

491
00:26:54,820 --> 00:27:01,680
His argument, and I'm simplifying a bit here, but not much, is that these models are just stochastic parrots.

492
00:27:01,680 --> 00:27:03,680
Stochastic parrots.

493
00:27:03,880 --> 00:27:05,020
It's such a great insult.

494
00:27:05,200 --> 00:27:05,920
So sticky.

495
00:27:06,340 --> 00:27:07,580
It sticks, doesn't it?

496
00:27:07,720 --> 00:27:10,180
Stochastic just means random or probabilistic.

497
00:27:10,820 --> 00:27:12,260
The idea is this.

498
00:27:13,180 --> 00:27:16,240
A parrot can learn to say, Polly wants a cracker.

499
00:27:16,880 --> 00:27:21,420
It can make the sounds perfectly, but the parrot doesn't know what a cracker is.

500
00:27:21,820 --> 00:27:23,280
It doesn't understand hunger.

501
00:27:23,520 --> 00:27:25,620
It doesn't understand the concept of wanting.

502
00:27:25,620 --> 00:27:27,340
It's just mimicry.

503
00:27:27,660 --> 00:27:28,900
It's not understanding.

504
00:27:29,180 --> 00:27:31,280
It's just associating sounds.

505
00:27:31,660 --> 00:27:31,780
Right.

506
00:27:32,080 --> 00:27:36,680
And the critique says that LLMs are doing the exact same thing on a massive scale.

507
00:27:37,020 --> 00:27:42,920
They have ingested the entire internet and they are just predicting the next word based on probability.

508
00:27:43,320 --> 00:27:47,840
They see the cat sat on them and they know Matt is a very likely next word.

509
00:27:47,840 --> 00:27:55,240
So they have system one, this ability to associate patterns really, really fast, but they lack system two.

510
00:27:55,360 --> 00:27:56,080
They can't reason.

511
00:27:56,240 --> 00:27:57,300
They can't stop and think.

512
00:27:57,480 --> 00:27:59,800
They were just autocomplete on steroids.

513
00:27:59,940 --> 00:28:00,680
That's the argument.

514
00:28:00,840 --> 00:28:02,100
I'm sure you've heard it a hundred times.

515
00:28:02,200 --> 00:28:03,280
It's not real intelligence.

516
00:28:03,500 --> 00:28:04,500
It's just statistics.

517
00:28:04,860 --> 00:28:06,840
It's a pattern matcher, not a thinker.

518
00:28:06,960 --> 00:28:10,020
But here comes Flitch in with aspect relegation theory.

519
00:28:10,140 --> 00:28:12,300
And he says, wait a minute.

520
00:28:12,980 --> 00:28:15,780
You are making a fundamental category error.

521
00:28:15,840 --> 00:28:17,100
A category error.

522
00:28:17,100 --> 00:28:17,900
Unpack that.

523
00:28:18,020 --> 00:28:19,200
Why is it an error?

524
00:28:19,740 --> 00:28:21,720
Well, think about what we just learned about humans.

525
00:28:22,400 --> 00:28:36,960
If system one is just compiled system two, if intuition is just reasoning that has been stabilized and compressed, then accusing AI of only having system one is a completely meaningless statement.

526
00:28:37,200 --> 00:28:41,020
Because you're basically accusing the AI of acting like a human expert.

527
00:28:41,260 --> 00:28:41,780
Exactly.

528
00:28:41,780 --> 00:28:47,420
If I watch a chess grandmaster play a game of speed chess, they are moving instantly.

529
00:28:47,880 --> 00:28:50,700
They aren't pausing for minutes to calculate every move.

530
00:28:50,820 --> 00:28:52,540
They are playing on intuition.

531
00:28:52,740 --> 00:28:54,940
They are using what feels like system one.

532
00:28:55,180 --> 00:29:01,720
If I told you that grandmaster isn't actually thinking, he's just pattern matching, you'd laugh at me.

533
00:29:01,720 --> 00:29:07,640
I'd say he's pattern matching because he has mastered the logic of chess over tens of thousands of hours.

534
00:29:08,060 --> 00:29:09,780
His pattern matching is his thinking.

535
00:29:10,100 --> 00:29:10,280
Right.

536
00:29:10,480 --> 00:29:14,000
His speed is proof of his competence, not proof of his stupidity.

537
00:29:14,360 --> 00:29:22,020
The source argues that when we see ChatGPT spit out a complex coding solution in three seconds, our reaction shouldn't be, it's not thinking.

538
00:29:22,020 --> 00:29:26,220
It should be, it is thinking at an incredibly high level of compression.

539
00:29:26,840 --> 00:29:33,360
So just because I can't see the AI stopping to think doesn't mean the logic isn't there embedded in the system.

540
00:29:33,560 --> 00:29:37,260
The author uses a programming analogy that is really, really helpful here.

541
00:29:37,780 --> 00:29:40,780
Think about source code versus machine code.

542
00:29:40,920 --> 00:29:41,120
Okay.

543
00:29:41,480 --> 00:29:42,820
Source code is what humans write.

544
00:29:43,200 --> 00:29:45,300
It's in Python or C plus tells me.

545
00:29:45,400 --> 00:29:45,620
Right.

546
00:29:45,740 --> 00:29:47,940
If you write a program in Python, it's readable.

547
00:29:47,940 --> 00:29:51,160
It says, if X is greater than five, then print the word hello.

548
00:29:51,900 --> 00:29:53,960
You can see the logical steps clearly.

549
00:29:54,820 --> 00:29:56,080
That is system two.

550
00:29:56,940 --> 00:29:58,020
It's explicit.

551
00:29:58,380 --> 00:29:59,300
It's slow to write.

552
00:29:59,480 --> 00:30:00,580
It's easy to understand.

553
00:30:01,180 --> 00:30:01,880
And machine code.

554
00:30:02,020 --> 00:30:02,360
What's that?

555
00:30:02,700 --> 00:30:08,220
Machine code is what happens after you compile the program so the computer can actually run it.

556
00:30:08,380 --> 00:30:12,180
It turns into a massive string of binary, zeros and ones.

557
00:30:12,440 --> 00:30:14,540
It's completely unreadable to a human.

558
00:30:14,760 --> 00:30:16,460
The if then statements are gone.

559
00:30:16,460 --> 00:30:20,260
The logic is like smeared across the entire file.

560
00:30:20,420 --> 00:30:22,440
It just executes instantly.

561
00:30:22,680 --> 00:30:25,000
But the logic is still inside the zeros and ones.

562
00:30:25,000 --> 00:30:27,720
It has to be or the program wouldn't work.

563
00:30:28,080 --> 00:30:28,680
Precisely.

564
00:30:29,160 --> 00:30:36,680
The author argues that criticizing AI for only having system one is like looking at the compiled machine code and saying,

565
00:30:36,980 --> 00:30:40,360
this isn't real computation because I can't read the source code anymore.

566
00:30:40,360 --> 00:30:48,900
The reasoning steps are hidden in the weights of the neural network, much like they are hidden in the expert driver's brain or the detective's gut.

567
00:30:48,900 --> 00:30:52,200
That is, wow, that really flips the script.

568
00:30:52,780 --> 00:30:58,840
It suggests that the stochastic parrot argument is actually punishing AI for being too efficient?

569
00:30:59,200 --> 00:31:02,640
It suggests we are mistaking fluency for lack of thought.

570
00:31:02,860 --> 00:31:06,680
We're mistaking the compiled executable for a dumb parrot.

571
00:31:06,680 --> 00:31:12,080
So does this mean the author thinks AI is perfect, that there's no problem here?

572
00:31:12,140 --> 00:31:22,120
Because I have definitely seen these models make some pretty stupid non-human mistakes, hallucinations, making up facts, getting basic logic wrong.

573
00:31:22,120 --> 00:31:24,200
And the source material absolutely admits that.

574
00:31:24,340 --> 00:31:25,840
It's not an AI hype piece.

575
00:31:25,980 --> 00:31:27,360
It's not saying AI is perfect.

576
00:31:27,480 --> 00:31:29,820
It's saying we are diagnosing the problem wrong.

577
00:31:30,180 --> 00:31:32,220
The problem isn't a lack of system two.

578
00:31:32,780 --> 00:31:36,320
The problem is a lack of dynamic regulation of resolution.

579
00:31:36,680 --> 00:31:39,100
Okay, bring that back to the commuter example for me.

580
00:31:39,340 --> 00:31:40,840
Think about our commuter again.

581
00:31:41,520 --> 00:31:45,980
When the subway entrance is closed, the commuter snaps out of it.

582
00:31:45,980 --> 00:31:51,180
They have an internal trigger, a feeling of surprise, of confusion that says,

583
00:31:51,560 --> 00:31:54,120
Hey, the autopilot is failing.

584
00:31:54,640 --> 00:31:57,400
Promote these aspects to system two immediately.

585
00:31:57,540 --> 00:31:58,220
Open the aperture.

586
00:31:58,580 --> 00:32:00,760
We need 4K resolution right now.

587
00:32:01,220 --> 00:32:03,280
The 240p stream is not working.

588
00:32:03,720 --> 00:32:04,100
Correct.

589
00:32:04,640 --> 00:32:07,260
Humans have what the author calls endogenous control.

590
00:32:07,680 --> 00:32:09,600
We can self-regulate our resolution.

591
00:32:09,880 --> 00:32:11,200
We know when we are confused.

592
00:32:11,600 --> 00:32:14,480
We feel it when a situation is weird or unexpected.

593
00:32:14,480 --> 00:32:16,340
And AI doesn't have that feeling.

594
00:32:17,280 --> 00:32:18,500
Currently, not really.

595
00:32:18,700 --> 00:32:20,260
AI is kind of stuck in one gear.

596
00:32:20,720 --> 00:32:24,760
It generates the answer to a complex physics riddle with the same resolution

597
00:32:24,760 --> 00:32:28,680
and the same level of confidence that it generates a poem about a cat

598
00:32:28,680 --> 00:32:30,420
or a simple coding solution.

599
00:32:30,720 --> 00:32:32,040
It doesn't have that internal,

600
00:32:32,500 --> 00:32:33,480
Wait a minute, this is tricky.

601
00:32:33,640 --> 00:32:37,060
I need to slow down and unpack my compressed reasoning trigger.

602
00:32:37,500 --> 00:32:41,720
So it's like a driver who is on autopilot and keeps driving at 60 miles per hour

603
00:32:41,720 --> 00:32:43,200
even though the bridge ahead is out

604
00:32:43,200 --> 00:32:45,640
because it doesn't have the internal mechanism to say

605
00:32:45,640 --> 00:32:48,340
situation changed, disengage autopilot.

606
00:32:48,900 --> 00:32:49,980
That is the perfect analogy.

607
00:32:50,180 --> 00:32:52,220
It lacks the metacognitive layer.

608
00:32:52,380 --> 00:32:53,600
It doesn't know what it doesn't know.

609
00:32:53,900 --> 00:32:55,740
It doesn't know when to widen the aperture.

610
00:32:56,060 --> 00:32:58,200
It's not that it can't reason in 4K.

611
00:32:58,420 --> 00:33:01,640
It's that it doesn't know when to stop streaming in 240p.

612
00:33:01,840 --> 00:33:03,500
That is such a crucial distinction.

613
00:33:04,020 --> 00:33:08,000
It completely shifts the goalpost for AI research, doesn't it?

614
00:33:08,000 --> 00:33:13,940
We don't need to build a logic module and try to bolt it onto the language module.

615
00:33:14,300 --> 00:33:16,620
We need to teach the system introspection.

616
00:33:17,060 --> 00:33:17,340
Yes.

617
00:33:18,020 --> 00:33:20,560
We need to teach it to recognize its own uncertainty.

618
00:33:21,020 --> 00:33:25,600
We need systems that can say, I don't know, or let me think about that,

619
00:33:25,700 --> 00:33:28,960
or let me expand my search and double check my work.

620
00:33:28,960 --> 00:33:32,640
The next breakthrough in AI won't be about getting faster.

621
00:33:33,180 --> 00:33:35,940
It will be about learning how and when to slow down.

622
00:33:36,300 --> 00:33:37,340
The ability to slow down.

623
00:33:37,420 --> 00:33:37,960
That's poetic.

624
00:33:38,280 --> 00:33:38,720
It is.

625
00:33:38,820 --> 00:33:44,860
It's about restoring the ability to access the source code when the compiled code fails you.

626
00:33:45,020 --> 00:33:46,920
I want to play devil's advocate for a second here

627
00:33:46,920 --> 00:33:50,260
because the source material anticipates some objections,

628
00:33:50,640 --> 00:33:53,320
and I think the listeners might be screaming at their advices right now

629
00:33:53,320 --> 00:33:55,000
about one specific thing.

630
00:33:55,060 --> 00:33:55,820
Lay it on me.

631
00:33:55,880 --> 00:33:56,360
I'm ready.

632
00:33:56,360 --> 00:34:01,580
We are saying all this system one stuff is just learned expertise.

633
00:34:02,160 --> 00:34:04,280
It's just relegated system two.

634
00:34:04,880 --> 00:34:06,380
But what about reflexes?

635
00:34:06,820 --> 00:34:09,980
If I hear a loud bang right behind me, I duck.

636
00:34:10,440 --> 00:34:13,500
If I touch a hot stove, I pull my hand away instantly.

637
00:34:14,180 --> 00:34:15,180
I didn't learn that.

638
00:34:15,340 --> 00:34:18,680
I didn't sit in a classroom and study loud bang theory.

639
00:34:18,860 --> 00:34:19,640
Loud bang theory?

640
00:34:19,740 --> 00:34:21,100
No, you definitely didn't.

641
00:34:21,180 --> 00:34:22,340
That wasn't compressed reasoning.

642
00:34:22,440 --> 00:34:23,460
That was just hardwired.

643
00:34:23,640 --> 00:34:25,540
I was born with that program.

644
00:34:25,540 --> 00:34:30,660
So doesn't that prove there is a separate fast brain, a lizard brain?

645
00:34:30,840 --> 00:34:34,780
This is the innate automaticity objection, and you are absolutely right.

646
00:34:34,860 --> 00:34:36,140
The source acknowledges this.

647
00:34:36,380 --> 00:34:38,820
There is such a thing as hardwired automaticity.

648
00:34:39,060 --> 00:34:40,820
Evolution gave you reflexes.

649
00:34:40,980 --> 00:34:42,460
That was never a system two.

650
00:34:42,760 --> 00:34:44,620
It's not a reduction sauce you cooked up.

651
00:34:44,620 --> 00:34:47,620
It's just raw ingredients you were born with.

652
00:34:48,100 --> 00:34:49,220
So doesn't that break the theory?

653
00:34:49,420 --> 00:34:52,360
Doesn't that prove there are two systems, one learned and one innate?

654
00:34:52,580 --> 00:34:56,640
The author argues, no, it doesn't break the theory for the purposes of this discussion.

655
00:34:56,900 --> 00:34:57,680
And here's why.

656
00:34:58,180 --> 00:35:03,120
The things we actually care about when we talk about intelligence, math, language, logic,

657
00:35:03,320 --> 00:35:09,160
strategy, chess, driving a car, diagnosing a disease, all of that stuff is relegated automaticity.

658
00:35:09,160 --> 00:35:10,360
Ah, I see.

659
00:35:10,760 --> 00:35:12,580
No one is born playing chess.

660
00:35:12,940 --> 00:35:15,920
No one is born speaking French fluently.

661
00:35:16,080 --> 00:35:16,600
Exactly.

662
00:35:16,900 --> 00:35:21,440
No one is born knowing how to navigate a subway system or write a podcast script.

663
00:35:21,620 --> 00:35:27,160
All the stuff that makes us smart, all the complex cognitive stuff we're trying to replicate

664
00:35:27,160 --> 00:35:32,500
in AI was learned through a process of moving from high resolution to low resolution.

665
00:35:33,240 --> 00:35:37,340
So for the context of intelligence and AI, the hardwired stuff is less relevant.

666
00:35:37,720 --> 00:35:39,960
The debate is about the learned capabilities.

667
00:35:40,420 --> 00:35:40,740
Got it.

668
00:35:40,960 --> 00:35:47,280
So we're distinguishing between biological reflexes, which are hardware, and cognitive intuition,

669
00:35:47,560 --> 00:35:49,940
which is software we wrote ourselves over time.

670
00:35:50,140 --> 00:35:50,660
Precisely.

671
00:35:50,900 --> 00:35:53,620
The theory of aspect relegation applies to the software.

672
00:35:53,620 --> 00:35:57,360
Okay, there's another objection here about metacognition, which we touched on.

673
00:35:57,500 --> 00:35:58,940
The idea of waking up.

674
00:35:59,240 --> 00:36:00,080
How do we do that?

675
00:36:00,160 --> 00:36:00,940
What does that trigger?

676
00:36:01,200 --> 00:36:02,140
This is the big mystery.

677
00:36:02,400 --> 00:36:07,300
The paper identifies this as the open problem for cognitive science and for AI.

678
00:36:08,240 --> 00:36:11,000
Humans have this incredibly rich set of triggers.

679
00:36:12,080 --> 00:36:16,100
Anxiety, surprise, cognitive dissonance.

680
00:36:16,260 --> 00:36:20,560
That feeling you get in your stomach when something just isn't right, when the story doesn't add up.

681
00:36:20,780 --> 00:36:21,040
Right.

682
00:36:21,040 --> 00:36:27,860
When you are driving and it suddenly starts raining really hard, you feel a physical tension.

683
00:36:28,160 --> 00:36:29,820
That effort signal kicks in.

684
00:36:29,980 --> 00:36:34,600
It forces you to grip the wheel, to pay more attention, to wake up from autopilot.

685
00:36:34,960 --> 00:36:39,660
And an AI, well, an AI doesn't have anxiety, doesn't get that stomach feeling.

686
00:36:39,800 --> 00:36:40,400
Not yet.

687
00:36:40,740 --> 00:36:48,120
The paper suggests that feeling effort or feeling confusion is the very mechanism that regulates our mental resolution.

688
00:36:48,120 --> 00:36:54,160
And since AI doesn't feel, in a biological sense, it doesn't know when to switch modes.

689
00:36:54,640 --> 00:36:58,900
So maybe the future of AI is we need to give it anxiety.

690
00:36:59,360 --> 00:37:00,760
That sounds like a terrible idea.

691
00:37:01,080 --> 00:37:02,520
In a functional sense, maybe.

692
00:37:02,640 --> 00:37:09,620
It needs an internal signal that says my current model of the world isn't matching the data coming in, error wake up, increase resolution.

693
00:37:09,620 --> 00:37:12,700
That is both terrifying and kind of amazing.

694
00:37:13,020 --> 00:37:19,680
The idea that Marvin the paranoid android from Hitchhiker's Guide might actually be the peak of AI evolution.

695
00:37:19,740 --> 00:37:24,560
It might be a necessary component for true, robust intelligence, a little bit of self-doubt.

696
00:37:24,780 --> 00:37:26,340
Okay, so let's bring this all home.

697
00:37:26,760 --> 00:37:29,200
What does this all mean for us, for the listener?

698
00:37:29,200 --> 00:37:32,140
We've dismantled the two-brain theory.

699
00:37:32,600 --> 00:37:37,000
We've realized our intuition is just zipped files of our own past work.

700
00:37:37,400 --> 00:37:39,880
We've realized AI might need anxiety.

701
00:37:40,560 --> 00:37:43,280
How does this change how I go about my Tuesday?

702
00:37:43,680 --> 00:37:45,300
I think there are two big practical takeaways.

703
00:37:45,520 --> 00:37:49,860
The first is about re-evaluating what we call lazy thinking.

704
00:37:49,860 --> 00:37:53,240
We are so hard on ourselves for being on autopilot.

705
00:37:53,380 --> 00:38:00,820
We read all these mindfulness books, and we think we should be present and logical and in system, too, 100% of the time.

706
00:38:00,960 --> 00:38:01,400
We do.

707
00:38:01,880 --> 00:38:05,220
But the source argues this is just cognitive economy.

708
00:38:05,560 --> 00:38:06,740
It is adaptive.

709
00:38:07,100 --> 00:38:08,080
It is smart.

710
00:38:08,260 --> 00:38:15,960
If you tried to walk to the subway in high-resolution mode every day, thinking about every muscle movement, every single paver on the sidewalk,

711
00:38:15,960 --> 00:38:20,000
you would collapse from exhaustion before you even got to the corner.

712
00:38:20,160 --> 00:38:21,740
You would completely crash the system.

713
00:38:21,860 --> 00:38:22,940
You have to relegate.

714
00:38:23,140 --> 00:38:23,840
You have to compress.

715
00:38:24,720 --> 00:38:32,620
Being on autopilot for the routine stuff is what allows you to have the bandwidth, the 4K streaming capability for the important stuff.

716
00:38:33,140 --> 00:38:35,740
You shouldn't try to be in system two all the time.

717
00:38:35,860 --> 00:38:36,860
That's not being smart.

718
00:38:37,000 --> 00:38:39,080
That's being profoundly inefficient.

719
00:38:39,600 --> 00:38:41,000
So embrace the blur.

720
00:38:41,540 --> 00:38:45,140
It's okay to watch the movie in 240p if it's a boring scene.

721
00:38:45,400 --> 00:38:45,880
Exactly.

722
00:38:46,480 --> 00:38:48,120
Save the 4K for the climax.

723
00:38:48,840 --> 00:38:52,240
Save your mental bandwidth for the decisions that actually matter.

724
00:38:52,240 --> 00:38:53,740
And the second takeaway.

725
00:38:54,120 --> 00:38:54,580
What's that?

726
00:38:55,120 --> 00:38:58,040
It's about moving away from this binary thinking.

727
00:38:58,260 --> 00:39:02,880
Stop thinking fast versus slow or logic versus emotion.

728
00:39:03,060 --> 00:39:03,400
Yes.

729
00:39:04,000 --> 00:39:08,220
Stop asking, am I being emotional or am I being rational right now?

730
00:39:08,720 --> 00:39:12,000
Start asking, what resolution does this problem require?

731
00:39:12,580 --> 00:39:13,380
It's a continuum.

732
00:39:13,700 --> 00:39:14,840
It's a sliding scale.

733
00:39:14,840 --> 00:39:16,760
It's a dial, not a switch.

734
00:39:17,000 --> 00:39:18,940
That is so much more helpful.

735
00:39:19,340 --> 00:39:21,120
Sometimes I feel like I need to be logical.

736
00:39:21,120 --> 00:39:29,440
But maybe I just need to turn up the dial a little bit, zoom in on one or two details, not try to flip a switch to a different brain.

737
00:39:29,440 --> 00:39:31,080
And it helps with learning, too.

738
00:39:31,080 --> 00:39:35,680
When you are learning something new, you have to accept that you must be in high resolution.

739
00:39:35,880 --> 00:39:36,680
It will feel hard.

740
00:39:36,820 --> 00:39:37,640
It will feel slow.

741
00:39:37,800 --> 00:39:39,520
It will feel deeply uncomfortable.

742
00:39:39,840 --> 00:39:40,720
That's the grind.

743
00:39:41,200 --> 00:39:42,900
That feeling of incompetence.

744
00:39:42,900 --> 00:39:44,280
That's the grind.

745
00:39:44,280 --> 00:39:47,660
But knowing this theory, you can reframe it.

746
00:39:48,080 --> 00:39:48,920
I'm not dumb.

747
00:39:49,180 --> 00:39:50,760
I'm just writing the source code.

748
00:39:51,920 --> 00:39:54,860
Eventually, with enough practice, it will compile.

749
00:39:55,560 --> 00:39:57,520
Eventually, it will become effortless.

750
00:39:57,520 --> 00:40:00,240
But you can't skip the coding phase.

751
00:40:00,760 --> 00:40:03,280
You can't just download the compiled file.

752
00:40:03,440 --> 00:40:05,960
You can't have the intuition without putting in the work first.

753
00:40:06,120 --> 00:40:09,500
You can't have the reduction sauce without the long simmer.

754
00:40:10,140 --> 00:40:10,620
Exactly.

755
00:40:11,080 --> 00:40:17,640
As we wrap up, I want to leave the listeners with a final thought from the source material that really, really twisted my brain.

756
00:40:18,480 --> 00:40:21,080
It's about the nature of consciousness itself.

757
00:40:21,220 --> 00:40:22,020
Oh, yes.

758
00:40:22,540 --> 00:40:24,180
This is the provocative end note.

759
00:40:24,300 --> 00:40:26,740
This is the real philosophical twist at the end.

760
00:40:26,740 --> 00:40:37,380
The source suggests that system two, that conscious, deliberative, hard thinking that we prize so much, the thing we think makes us human, isn't actually the superior mode of thinking.

761
00:40:37,580 --> 00:40:38,540
It's not the goal.

762
00:40:38,820 --> 00:40:41,740
It argues that consciousness is just the training mode.

763
00:40:42,060 --> 00:40:48,080
It implies that conscious thought is the clumsy, inefficient state of not having mastered a subject yet.

764
00:40:48,420 --> 00:40:49,660
Just think about that for a second.

765
00:40:49,660 --> 00:41:00,120
When you are perfect at something like walking or speaking your native language or a musician playing a song they've practiced 10,000 times, you are unconscious of it.

766
00:41:00,620 --> 00:41:01,320
You just do it.

767
00:41:01,320 --> 00:41:08,820
It's only when you are bad at it or learning it or fixing a mistake that you have to become conscious and deliberate.

768
00:41:09,540 --> 00:41:22,220
So if we follow that logic to its absolute limit, a perfect mind, a mind that had mastered everything, a godlike intelligence would be entirely unconscious.

769
00:41:22,220 --> 00:41:31,520
It would be pure automaticity, pure, effortless, relegated system one, no inner monologue, no feeling of effort.

770
00:41:31,860 --> 00:41:36,760
It completely questions our entire assumption that consciousness is the pinnacle of evolution.

771
00:41:37,520 --> 00:41:45,240
Maybe consciousness is just the scaffolding we use to build our habits and our expertise, and once the building is done, we are supposed to take the scaffolding down.

772
00:41:45,460 --> 00:41:48,620
Maybe the goal of all our thinking is to finally be able to stop thinking.

773
00:41:48,620 --> 00:41:53,620
That is a thought worth meditating on, or perhaps not meditating on.

774
00:41:53,680 --> 00:42:00,280
Well, on that absolute existential cliffhanger, I'm going to go relegate some aspects of my commute home.

775
00:42:00,540 --> 00:42:02,100
I'm going to try to enjoy the blur.

776
00:42:02,360 --> 00:42:03,460
Try not to overthink it.

777
00:42:03,720 --> 00:42:05,440
Thanks for listening to The Deep Dive.

778
00:42:05,580 --> 00:42:06,500
We'll catch you next time.

