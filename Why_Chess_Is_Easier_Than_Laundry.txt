Welcome back to the Deep Dive. Today, we are doing something a little bit different.
Oh, yeah.
Yeah, we're not just looking at a piece of tech or, you know, some historical event.
We are wrestling with a ghost.
A ghost? That is a very dramatic way to start.
It is, isn't it? But honestly, after spending the last few days with the paper,
that's exactly what it feels like.
Okay, I'm intrigued. What's the ghost?
We're talking about difficulty.
You know, that visceral feeling when you're trying to solve a math problem
that just won't crack or learn a new language.
And this is a big one for me personally.
Trying to fold a fitted sheet without it looking like a crumpled ball of despair.
The nemesis of laundry folders everywhere. The final box.
Exactly. But here's the thing.
We instinctively feel like difficulty is this heavy thing.
We treat it like a physical property.
Like calculus is heavy and tic-tac-toe is light.
We treat it like weight that, you know, lives inside the task itself.
Right. We treat difficulty like it's gravity.
It's just there, an intrinsic property of the universe that you have to overcome.
But we are looking at a paper today that says, absolutely not.
That entire way of thinking is wrong.
Not just a little bit off.
No, not just slightly off. It's fundamentally backwards.
The paper is titled, Noun-Free Cognition, Difficulty, Abstraction, and the Mobility of Computation
by Flickshannon, published just this year, 2026.
Right.
And it starts with this central mystery that has bugged me for years.
Why are experts so incredibly bad at predicting what computers will find hard?
The paradox of predictable unpredictability.
This is a fascinating place to start because it really, um, it exposes our arrogance as a species.
It really does.
I mean, think back to the 1990s.
Think about Deep Blue versus Kasparov.
Oh, yeah. Huge deal.
The entire world held its breath.
We truly believe that if a machine could conquer chess, this fortress of logic, strategy, and foresight,
that intelligence was solved.
They were at the finish line.
We thought the rest folding shirts, walking upstairs, common sense, was just the easy cleanup work.
We thought those things were trivial.
We assume that because we do them without thinking, they must be, you know, computationally simple.
Right.
We mistook a calculator for a brain.
Yeah.
And now...
Well, now it's a completely different story.
My phone plays better chess than any human who has ever lived.
It can beat a grandmaster while I'm scrolling through social media.
But if I ask a robot, even a billion-dollar cutting-edge robot, to walk into a messy room and just tidy up or fold a warm towel...
It has a nervous breakdown.
It freezes.
Freezes.
It's the classic chess versus laundry paradox.
We assume chess was hard and laundry was easy because we were judging difficulty based on what we find hard.
We projected our own biological struggles onto the machine.
And this paper argues that the reason we keep getting this wrong, why we have self-driving cars that can navigate highways at 70 miles per hour but get confused by a traffic cone...
Right.
...is because we think difficulty is a noun.
We think it's a thing.
And the mission today for this deep dive is to dismantle that.
We need to stop seeing difficulty as a noun, as a static weight, and start seeing it as a relation, a moving target.
A ghost that moves through the system.
Precisely.
If we can get our heads around this, we stop asking, why is this hard?
And we start asking a much more interesting question.
Which is?
Where did the difficulty go?
So let's unpack this difficulty is not weight idea because I have to be honest, when I'm lifting a 50-pound dumbbell, that's 50 pounds.
Sure.
Feels that way.
It doesn't matter if I'm tired.
It doesn't matter if I'm happy.
It's 50 pounds.
Gravity doesn't care about my feelings.
Yeah.
Are you saying a math problem isn't like a dumbbell?
That is exactly what Flickshan is arguing.
If difficulty were like weight, it would be intrinsic.
It would live inside the math problem.
But let's stick with that dumbbell for a second.
Okay.
Is it heavy?
Yes.
50 pounds.
Is it heavy to a forklift?
Well, no.
To a forklift, it's nothing.
Is it heavy if you are floating in the International Space Station?
Okay, I see where you're going with this.
It's weightless there.
The mass is the same.
The heaviness, the difficulty of lifting it, changed entirely because the context changed.
The paper introduces a relational model.
It says difficulty is actually a function of four variables interacting at once.
You can't look at just the object.
Okay, lay them on me.
What are the four variables?
First, you have the task.
That's the abstract goal, like get from point A to point B.
Okay, the task.
Second, you have the system.
That's you, or the computer, or the forklift, the thing doing the work.
System.
Got it.
Third, and this is crucial, you have the prompt.
We'll dig into that one.
It's a prompt.
And fourth.
The environment.
The context, the time, the energy available.
Okay, let's break these down because usually we just lump them all together.
Driving is hard.
But under this model, driving isn't the difficulty.
Exactly.
Let's take driving.
The task is navigate the vehicle to the destination.
Pretty simple, abstractly.
Sure.
The system is a human with eyes, hands, and a brain that's evolved for certain things.
The environment is the road conditions.
So, driving a car is easy on a sunny day.
The task is the same.
I'm the same.
But let's say a blizzard hits.
Suddenly, the environment shifts.
And what happens to the difficulty?
It spikes.
Through the roof.
My knuckles turn white.
I turn off the radio.
I'm sweating.
But did the task change?
You're still just trying to get to the destination.
No.
Same task.
Did the car change?
No.
The difficulty emerged from the friction between the system, you, and the environment.
The snow.
It's a mismatch.
It is a profound mismatch.
And the paper goes deeper.
It argues that we constantly mistake our own compilations, our habits and tools, for the task itself.
Okay, what do you mean by compilations?
Think of them as shortcuts.
Habits.
Muscle memory.
When the environment shifts, those tools stop working.
And that mismatch is what we experience as hard.
So difficulty isn't a thing you encounter.
It's a signal that your tools just broke.
That's a great way to put it.
It's an error message from your brain.
It's the sound of your internal software crashing against a new reality.
I want to dig into that third variable you mentioned, the prompt.
Because, honestly, when I hear prompt in 2026, I immediately think of typing into an AI chatbot.
Of course.
Write me a poem about a sad toaster.
Generate an image of a cat in a spacesuit.
That's a prompt.
And that is exactly the cultural baggage this paper wants to strip away.
We have narrowed the definition of prompt so much that we're missing its actual function in cognition.
How so?
If we only think of prompts as text we type into a box, we miss how the entire physical world prompts us constantly.
So, de-narrow it for me.
What is a prompt in this noun-free world?
A prompt is any specification that partitions the world into given and unresolved.
Whoa, okay.
It's a boundary condition.
It draws a line in the sand and says, everything behind this line is resolved.
You don't need to think about it.
Everything in front of the line, that's your problem.
Okay.
That's a bit abstract.
Partitions the world.
Can you give me a concrete example that isn't a chatbot?
Think about a blueprint for a house.
Okay.
A big roll of paper with blue lines.
A blueprint is a prompt.
Think about the cognitive load required to build a house.
You need to know the load-bearing capacity of the wood, the geometry of the roof pitch, the flow of the plumbing, the electrical layout.
Right.
It's massive.
If a builder had to figure all that out from scratch on the morning of the build, nothing would ever get built.
Exactly.
Honestly, when a builder looks at a blueprint, the geometry of the house, the dimensions, the layout, that is all given.
It's resolved.
It's resolved structure.
The builder doesn't have to derive the math of the roof pitch or decide where the bathroom goes.
That cognitive load has been relegated to the paper.
The paper is doing the thinking.
The paper is holding the frozen thinking of the architect.
The blueprint says, don't worry about the math, just cut the wood to this length.
The builder's unresolved task is just the physical execution.
So the blueprint prompts the construction by removing the need to be an architect in real time.
It's like a time capsule of decision making.
Yes.
It's an act of relegation.
And once you see this, you see prompts everywhere.
There else.
Take a musical score.
Sheet music.
Think about what a musician is doing.
If you didn't have the score, you'd have to compose the music while playing it.
You'd have to decide the melody, the rhythm, the harmony, the dynamics, all in real time.
That's improvisation.
And it's incredibly hard.
It is a huge cognitive load.
But the sheet music is a prompt that relegates the decision of what note comes next and how long do I hold it.
I see.
The composer took on the structural difficulty so the performer can focus on the performance difficulty, the emotion, the timbre, the timing.
That's fascinating.
So a prompt is basically a way of saying, here, ignore all this stuff so you can focus on this one little sliver.
Yes.
And here's the really cool part.
The paper argues that physical objects can be prompts too.
Wait, how can a physical object be a prompt?
Have you ever done any woodworking or even just assembled, you know, IKEA furniture?
I built some bookshelves.
I used a jig to drill the holes.
Perfect example, a jig.
If you need to drill a hole in exactly the same spot on 10,000 boards, you could try to measure it every time with a ruler and a pencil.
That would take forever.
And I'd probably mess up half of them because my hand would shake or I'd misread the ruler.
Right.
That's a high difficulty task requiring intense focus, steady hands, and constant calculation.
But what does the jig do?
It's a clamp with a guide hole.
You just shove the wood in until it hits the stopper and put the drill in the hole.
Boom.
Done.
The jig is a physical prompt.
It resolves the spatial alignment.
It relegates the need for hand stability.
The difficulty is just gone.
The task drill the hole is technically the same, but the difficulty has vanished because the prompt changed the boundary conditions.
This connects to something the paper mentioned about the interface of difficulty.
It said that changing the prompt changes the difficulty without changing the task.
Right.
Imagine a spatial logic puzzle.
If I describe it to you in text, block A is left of block B, which is above block C, it's really hard to solve.
Your brain has to simulate the space.
I hate those.
Susan sits next to the person wearing red, but not opposite the person eating fish.
My brain just shuts down.
It feels impossible.
But if I give you a diagram, a picture of the table with empty slots.
Easy.
Instant.
I can just see it.
I can just move things around.
The task is identical.
Find the seating arrangement.
But the diagram prompt aligns with your visual processing system.
It selects a pre-compiled affordance, your ability to see patterns instantly.
And the text prompt.
It mismatches your toolkit.
It forces you to use your much slower, more energy-intensive logical brain.
So is the puzzle hard?
Or did I just give you a bad prompt?
That is the key takeaway here, isn't it?
Yeah.
Prompts select our tools.
Yes.
If the prompt matches the tools we already have, our pre-compiled affordances, it feels easy.
If it misses, it feels impossible.
Which leads us directly into the mechanism of how we build those tools in the first place.
The paper calls this abstraction as compilation.
I have to admit, this is where I stumbled a bit in the reading.
I know compilation from computer science.
You write code in, like, English-ish words, and the compiler crunches it down into ones and zeros that the machine can run instantly.
Correct.
It turns a complex logical argument into a set of automatic instructions.
It bundles it up.
But I'm not a computer.
I'm not running code.
Yeah.
So how does this apply to me driving to the grocery store?
Think about when you first learned to drive.
Do you remember that first day in the parking lot?
Oh, God, yes.
It was a nightmare.
I was thinking about everything.
How much pressure is on the gas?
Where are my hands?
Am I too close to the curb?
What did the mirrors show?
You were running interpreted code.
You were processing every single line of data in real time.
High cognitive load.
High energy.
You were exhausted after 20 minutes.
Drenched in sweat.
It was awful.
But now you drive to work and sometimes you don't even remember doing it.
You arrive and think, wait, did I stop at all the red lights?
Ideally, I did.
But yes, I know the feeling.
It's automatic.
It's just driving.
That is because you have compiled the process.
Press pedal is now a single atomic action for you.
You are not thinking about the combustion engine, the fuel injection, the friction of the tires, or the hydraulic pressure in the brake lines.
You have an interface, the pedal, and you trust that the underlying machinery will handle the physics.
So my brain has zipped up all that complexity into a single file called go.
Exactly.
But here is the danger.
And the paper calls this the informal theorem.
No compilation remains optimal forever.
Because unlike software, where the hardware doesn't change much, reality is a moving target.
Right.
The pedal abstraction works great on dry asphalt.
The go file executes perfectly.
But let's go back to that snowy day we talked about.
Suddenly, press pedal does not equal go forward.
It equals spin out and die.
The compilation fails.
The abstraction leaks.
It breaks.
And this is that moment of panic.
You have to decompile.
You have to suddenly remember that the car is a physical object interacting with a slippery surface.
You have to pump the brakes, steer into the skid.
You have to think about the physics again from first principles.
And that re-exposure of the internal structure is what we experience as a sudden spike in difficulty.
The paper suggests that expertise is actually a form of fragility because of this.
Wait, hang on.
Expertise is fragility.
I thought experts were the robust ones.
If I hire an expert driver, I expect them to handle the snow better than me.
They can handle the snow because they have a compilation for snow.
That's a pattern they've compiled.
But think about it structurally.
Experts are efficient because they have deep stacks of these compiled shortcuts.
Okay.
They can ignore 99% of the information because they have a prompt or a mental model that handles it.
They see a pattern and they just run the snow program.
But if the context shifts in a way that violates their hidden assumptions...
Like a grandmaster chess player suddenly having to play on a board where the squares change color and gravity shift.
Oh, wow.
They often crash harder than a novice.
Because the novice never had the shortcut to begin with.
Exactly.
The novice was already looking at the raw physics.
They were already struggling.
So the change in context isn't as jarring.
The expert has to unlearn their shortcuts before they can even start solving the new problem.
That makes so much sense.
It's like when a software update changes the location of a button I use every single day.
Right.
I'm paralyzed for like 10 minutes trying to find it.
My mom, who never learned where the button was in the first place, just looks for it and finds it.
Precisely.
Your expertise, your muscle memory, became an obstacle.
It was a brittle compilation.
This really explains the chess vs. Tetris thing we started with.
This asymmetry of intelligence.
The paper argues that chess was a socio-historical artifact.
It's a bit of a burn on chess, honestly.
It is.
It basically says chess became the benchmark for intelligence just because it was hard for us in a very specific human way.
It was hard because it required combinatorial, search-looking, many moves ahead.
If I move here, he moved there, then I move there.
Our working memory is small.
It's tiny.
We can't hold that many future states in our heads.
That is difficult for a biological brain that evolved to hunt and gather and read facial expressions.
Not to play chess.
Not to play chess.
But for a computer, that kind of search is trivial.
It's just math.
It's a closed system with rigid rules.
Once we figured out the structure, we could externalize it.
We could write a prompt.
The chess engine that solved it.
So once the structure aligned with the compilation, the difficulty just vanished.
It wasn't inherently hard.
It was just hard for meat brains.
Precisely.
But look at Tetris.
Or even better, let's go back to folding that towel.
Why is the towel so hard for the robot?
I've watched videos of robots trying to fold laundry.
It's painful.
They move so slowly.
They poke at it.
They get tangled.
Why?
What's the core problem?
I guess because the towel is soft.
It flops.
It changes shape every time you touch it.
Right.
It relies on real-time perception, motor loops, feedback from your fingertips.
There is no rigid grid.
You cannot just calculate the towel.
You have to feel it.
And humans have millions of years of evolution embodied compilations that handle that.
We have compiled grasping a soft object so deeply that we don't even know how we do it.
Try to explain to a robot how to hold a towel without crushing it or dropping it.
Okay.
Don't squeeze too hard, but don't squeeze too soft.
That means nothing to a robot.
That's a useless prompt.
It involves millions of micro-adjustments per second based on tension sensors in your skin.
For the machine, the towel is an uncompiled nightmare of physics simulations.
For us, it's just... grab.
This is the no final convergence argument.
There is this belief in tech, I hear it all the time in Silicon Valley, that AI will eventually
just catch up and everything will be easy for everyone.
Machines will do what we do, we'll do what they do.
And this paper says, no, that's a fundamental misunderstanding.
There will always be an asymmetry.
Because our histories are different.
Our compilations are different.
Yes.
We are compiled for the savanna, for social nuance, for manipulated physical objects.
Machines are compiled for symbol manipulation and massive data processing.
We will always find different things hard because we are running on different legacy code.
That makes so much sense.
We aren't converging on some universal intelligence.
We're just distinct systems with different easy buttons.
Exactly.
And this leads to a massive misunderstanding of technological progress.
How so?
We tend to think technology eliminates difficulty.
We invented the dishwasher, so washing dishes is solved.
We invented the internet, so communication is solved.
But the paper argues for the conservation of complexity.
Or rather, computational displacement.
Right.
It's not that the difficulty is destroyed.
That's impossible.
It's redistributed.
It has to go somewhere.
The example of the one-click purchase really hit home for me.
On my end, as the user, I tap a glass screen.
One second.
Zero difficulty.
Low assembly index for you?
It's effortless.
But for that one tap to result in a package at my door the next day, let's trace that difficulty.
Where did it go?
It didn't vanish.
It exploded outward.
To make your experience simple, the system had to absorb massive complexity.
You need global logistics networks tracking millions of items in real time.
You need server farms cooling themselves in the desert, consuming small cities' worth of electricity.
I like it.
You need cybersecurity teams fighting off hackers 24-7.
You need version control for the app software, A-B testing, user support.
It moved into the infrastructure layer.
And it moved on to other people.
Think about the warehouse worker.
That's the hardening of the analog world the paper talks about.
Yes.
When you make the digital interface smooth and frictionless, you often make the physical reality harder, more brutal.
The warehouse worker's job shifts from craftsmanship, or storekeeping, to becoming a component in the machine's logic.
They're chasing the algorithm.
The robot tells them where to go, how fast to walk, when to take a break.
Their difficulty, the physical toll on their body increases to support my ease.
The complexity got displaced onto them.
And for the rest of us, look at the physical toll of these easy interfaces.
We have interfaces optimized for symbols, screens, keyboards.
And our bodies are screening.
Carpal tunnel, tech neck, eye strain, anxiety.
That is the difficulty returning to us in a different form.
We removed the difficulty of walking to the store or hunting for food, but we replaced it with the difficulty of sedentary lifestyle management and attention fragmentation.
Oh, the attention fragmentation is real.
The paper calls this the paradox of efficiency.
Exactly.
Making things easier to start lowering the friction means we do them more often.
I feel this with email.
If sending an email cost $5 and took an hour to handwrite, I would send maybe one a week.
I'd be thoughtful.
But since it's free and instant.
I get $500 a day.
And I'm expected to answer them all.
My entire day is managing this easy task.
So what was hard to do, writing a letter is now easy.
But what was easy managing your correspondence is now impossible.
The difficulty shifted from execution to management.
We are drowning in volume because we remove the friction of entry.
This feels like a perfect segue into section five, the trap of metrics.
Because how do we try to manage this volume?
How do companies manage the 500 emails or the warehouse efficiency?
We measure it.
We count the emails.
Yeah.
We track productivity.
And we run headfirst into Goodhart's Law.
Now, usually when people talk about Goodhart's Law, when a measure becomes a target, it ceases to be a good measure.
They treat it like a data problem.
Right.
Oh, we just picked the wrong KPI.
If we find the right number, it'll work.
But Flickshaw argues it's an evolutionary force.
It's almost biological.
It's about turning a process into a noun.
Unpack that for me.
Process to noun.
Okay.
Learning is a process.
It's fluid.
It's messy.
It happens in your head.
It involves failure and confusion and insight.
But a bureaucracy can't manage learning.
It's too ghostly.
It's invisible.
So they turn it into a noun.
A test score.
Or a line of code.
Or a number of tickets closed.
These are static snapshots.
And once the system sets that noun as the goal, every actor in the system starts optimizing for the noun, not the process.
This reminds me so much of my time working in a call center right out of college.
Oh, I bet.
We had a metric.
Average handle time.
We had to keep calls under three minutes.
That was the noun.
Let me guess what happened.
Did you become incredibly efficient at solving complex problems in under 180 seconds?
No.
We started hanging up on people.
If a problem sounded hard, we'd accidentally lose the connection.
Or we'd solve the easy part and tell them to call back for the rest.
You gamed the metric.
We optimized the metric perfectly.
Our times were great.
But the underlying process, actually helping customers, was destroyed.
Completely.
Customer satisfaction must have tanked.
It fell off a cliff.
And that is the optimization death spiral.
Because what did management do when they saw satisfaction drop?
Did they remove the metric?
No, of course not.
They added a new layer.
They added a new layer.
They said, okay, you have to keep calls under three minutes, but the customer also has to give you a four-star rating.
And now you have two metrics to game.
You just beg people for good ratings.
The complexity of the system has net increased.
You have added rules to patch the holes created by your previous simplification.
This explains why bureaucracies and tech stacks always get bloated.
They are patching their own abstractions.
They are chasing the ghost of the process they killed by turning it into a noun.
That is actually kind of tragic.
It is.
It's a tragedy of structure.
Nobody is evil in that scenario.
The system just forces that behavior.
But why do we do this?
It seems like there is a physics to this behavior.
Why do we always take the shortcut?
Why do we always game the metric?
The paper uses assembly theory to explain this.
Now, we don't need to get into the heavy math, but the core principle is the assembly index.
Which is basically, how many steps does it take to build this thing from its parts?
Right.
But the key insight is about selection.
The rule of adaptive systems, whether it's evolution or a chemical reaction or a corporate team,
is that they do not look for the global best solution.
They don't look at the map and plan the optimal route.
No.
They look for the immediate lowest resistance.
Imagine water flowing down a hill.
Okay.
The water doesn't look at the landscape and say,
oh, if I go slightly uphill here over this ridge, I'll find a much faster route to the ocean later.
No.
It just goes down.
Immediately.
It follows the slope right in front of it.
It takes the next easiest step.
Even if that step leads it into a swamp instead of the ocean.
Exactly.
And in cognition, that means using a compiled abstraction.
Using a shortcut.
So if I'm a programmer, using a pre-made software library is easier now.
It lowers my immediate assembly index.
I don't have to write the code myself.
But that library couples you to someone else's bugs.
Yeah.
It bloats your software.
It creates technical debt.
That is the shortcut trap.
We lower the immediate effort, but we raise the long-term maintenance cost.
We flow downhill into a swamp of complexity.
And here's the scary part.
Technological progress just gives us more shortcuts.
Oh, right.
Greater capability expands the number of pre-compiled libraries, tools, and AIs we can reach for.
So we can build things faster, low immediate resistance.
But we are building them out of black boxes we don't understand.
So we are accelerating the redistribution of complexity into invisible layers.
We are building a skyscraper out of bricks we didn't bake on a foundation we didn't pour.
And wondering why it sways in the wind.
This connects to the philosophy section of the paper, which I found surprisingly deep.
The saying versus doing gap.
This was in the appendix, but I found it profound.
It connects to Ludwig Wittgenstein, the ghost of Wittgenstein.
He's the meaning is use guy, right?
Language games.
Yes.
He argued you can't define game or difficulty in a vacuum.
The meaning comes from playing the game, from the context.
But the paper adds this corollary.
Saying is compression.
Doing is construction.
Saying is a pointer.
What does that mean?
Think about generative AI.
Think about mid-journey or DALI.
Okay.
If I type the prompt, show me a photorealistic image of a suspension bridge connecting New York to London, the AI generates it in seconds.
And it looks incredible.
It has cables, towers, water, lighting.
It's beautiful.
But is it a bridge?
No.
It's a picture.
It's worse than a picture.
It's a pointer.
If you tried to build from that image, the bridge would collapse instantly.
The AI said bridge, but it didn't do bridge.
It didn't resolve the gravity, the tension, the wind shear, the steel ratings, the cost of materials, the bedrock on the ocean floor.
It pointed to the idea of a bridge without doing the work of the bridge.
Exactly.
Saying uses a pointer to a region of possibility space.
Doing requires building the causal chain to get there.
It requires resolving every single unresolved dependency.
The paper argues this asymmetry is structural.
It will always be easier to say something than to do it.
Because language is a lossy compression of reality.
It strips away the constraints.
And as our tools improve, as we get better AI, we can say things with even higher fidelity.
We can generate plans that look incredibly detailed.
So the saying has become incredibly sophisticated.
It looks like doing.
But it isn't.
The image is still just a pointer.
It has zero structural integrity.
This explains why project timelines are always wrong.
Ideas are cheap.
Ideas are cheap because they are nonified possibilities.
They are just pointers.
Execution is the re-exposure of all the hidden difficulty.
And the paper suggests this gap is widening.
We can imagine and prompt systems that are vastly more complex than what we can physically manage or execute.
That is daunting.
It feels like we are trapping ourselves in a hall of mirrors where everything looks easy but nothing actually works.
It can feel that way.
It's a real danger.
So where does this leave us?
I mean, if difficulty is this moving target, this relation, this ghost, if our metrics are lying to us and our shortcuts are traps, what does it mean to be smart?
This is Section 8, Implications for Intelligence.
We have to redefine it.
The old definition was capacity to solve inherently hard problems, like chess.
Which we know now doesn't exist.
There are no inherently hard problems.
There are only problems that mismatch your current tools.
So the new definition, what is it?
Intelligence is the capacity to navigate shifting boundaries.
It is the ability to recompile when the environment changes.
I love that.
It's not about how fast you can run the code.
It's about how fast you can rewrite it when the hardware melts.
Exactly.
It's about fluidity and adaptation.
And the paper defines stupidity in a very interesting way, not as a lack of brain power.
But as an overcommitment to obsolete abstractions.
Being stuck in your old ways, insisting that the pedal still works even though you're on ice, that is stupidity.
It's a rigidity of compilation.
This has huge ethical consequences, too.
The paper talks about fairness.
It does.
If difficulty is a relation between the task, the prompt, and your pre-compiled affordances, then you cannot judge two people by the same output.
Because one person might have a scaffold.
A better set of tools.
Exactly.
If I grew up with tutors, high-speed internet, a stable home, and fluent English, I have a massive stack of compiled abstractions that resolve difficulty for me.
I can focus on the high-level task because the low-level survival stuff is given.
It's been humbled for me.
And if someone else is dealing with food insecurity, or a second language, or a noisy home, or lack of access?
They are decompiling survival every single day.
They are spending their cognitive energy on the given.
So a task that is easy for me is hard for them.
Not because I am smarter, but because my scaffolding is doing the heavy lifting.
So fairness isn't about treating everyone the same.
It's about looking at the compilation history.
And recognizing that responsibility lies in the design of the system.
The design of the prompts.
Are we designing prompts that only work for people with a specific set of tools?
That is a powerful shift.
It moves the blame from the individual to the architecture.
It forces us to ask, who is bearing the displaced complexity of our simple systems?
So we've been on a journey.
We started with the paradox of the robot folding laundry.
The things we thought were easy are actually hard.
Right.
We realized difficulty isn't a weight.
It's a relation between the system and the environment.
We learned that prompts are boundaries that partition the world into given and unresolved.
We saw that abstraction is just compilation that eventually breaks when the road gets icy.
And that that's what difficulty feels like.
We tracked the movement of complexity from the user interface into the infrastructure in the human body.
And we saw how metrics turn processes into nouns and break them.
A tour of noun-free cognition.
So what now?
The paper ends with a provocation.
A so what?
And it uses a metaphor that I think is perfect for wrapping this up.
The ladder.
The ladder metaphor.
It's a nod to Wittgenstein again.
He said that his philosophy was like a ladder.
You use it to climb up to a new vantage point.
But once you're up there...
You can throw the ladder away.
You don't need it anymore.
You don't need to walk around reciting assembly index minimization or prompt boundary conditions in your daily life.
You don't need the jargon.
You just need the view.
You just need to change your seeing.
Exactly.
The goal isn't to become a philosopher of difficulty.
The goal is to notice the ghost.
And the call to action is simple but radical.
Stop trying to eliminate difficulty.
It cannot be destroyed.
That is the fundamental law.
It can only be moved.
And the goal isn't a frictionless world.
No.
That's a fantasy that leads to fragility and disaster.
The goal is to consciously manage where you put the friction.
Do you want the friction on the user, on the worker, on the environment, on the future?
Because it has to go somewhere.
The choice is where.
So, to everyone listening, here's your final thought.
The next time you are struggling with a task,
whether it's a spreadsheet that won't balance a difficult conversation with a partner,
or, yes, folding that fitted sheet,
don't ask, why is this hard?
Because this doesn't have a difficulty.
Instead, ask, which of my compiled abstractions just stopped working?
Or, even better, who moved the complexity onto me?
That is the question.
Thanks for diving deep with us.
We'll see you in the next one.
Keep recompiling.
Keep compiling.
Keep compiling.
